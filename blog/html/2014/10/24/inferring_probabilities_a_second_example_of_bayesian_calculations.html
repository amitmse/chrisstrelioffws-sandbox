<!DOCTYPE html><!--[if lt IE 7]>      <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js"> <!--<![endif]-->
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="description" content="Christopher Strelioff's blog">
        <meta name="viewport" content="width=device-width">
        <title>Inferring probabilities, a second example of Bayesian calculations &mdash; chris&#39; sandbox</title>
            <link rel="stylesheet" href="../../../_static/normalize.css" type="text/css">
            <link rel="stylesheet" href="../../../_static/sphinx.css" type="text/css">
            <link rel="stylesheet" href="../../../_static/main.css" type="text/css">
            <link rel="stylesheet" href="../../../_static/flat.css" type="text/css">
            <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
            <link rel="stylesheet" href="../../../_static/font-awesome.min.css" type="text/css">
        <link rel="stylesheet" href="../../../_static/style.css" type="text/css" /><link rel="shortcut icon" href="../../../_static/tinkerer.ico" /><!-- Load modernizr and JQuery -->
        <script src="../../../_static/vendor/modernizr-2.6.2.min.js"></script>
        <script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.2/jquery.min.js"></script>
        <script>window.jQuery || document.write('<script src="../../../_static/vendor/jquery-1.8.2.min.js"><\/script>')</script>
        <script src="../../../_static/plugins.js"></script>
        <script src="../../../_static/main.js"></script>
        <link rel="next" title="Python 3.4 on Ubuntu 14.04 using virtual environments" href="../../09/16/python_3_4_on_ubuntu_14_04_using_virtual_environments.html" /><link rel="alternate" type="application/rss+xml" title="RSS" href="../../../rss.html" /><script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     '1.4.2',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script><script type="text/javascript" src="../../../_static/underscore.js"></script><script type="text/javascript" src="../../../_static/doctools.js"></script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript" src="../../../_static/disqus.js"></script><script type="text/javascript" src="../../../_static/google_analytics.js"></script>

    <script type="text/javascript">
        $(document).ready(function () {
            // Scroll to content if on small screen
            if (screen.width < 480)
            {
                $(document).scrollTop(document.getElementsByTagName("article")[0].offsetTop - 44);
            }
        });
    </script></head>
    <body role="document">
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> or <a href="http://www.google.com/chromeframe/?redirect=true">activate Google Chrome Frame</a> to improve your experience.</p>
        <![endif]-->

      <div id="container"><header role="banner">
    <hgroup>
      <h1><a href="../../../index.html">chris&#39; sandbox</a></h1><h2>python, R, ubuntu, bayesian methods, machine learning and more...</h2></hgroup>
  </header>
<div class="main-container" role="main"><div class="main wrapper clearfix"><article><div class="timestamp postmeta">
            <span>October 24, 2014</span>
        </div>
    <div class="section" id="inferring-probabilities-a-second-example-of-bayesian-calculations">
<h1>Inferring probabilities, a second example of Bayesian calculations</h1>
<p>In this post we will focus on an example of inferring probabilities given a
short data sample.  We will start by tackling the theory of how
to do the desired inference in a Bayesian way and will end by implementing
the theory we develop in Python so that we can play around with the ideas.  To
keep the post (hopefully) more accessible, we will only consider a small set of
candidate probabilities. This restriction allows us to minimize the
mathematical difficulty of the inference and still obtain really cool results.</p>
<div id="more"> </div><p>If the content below seems unfamiliar try reading previous posts that provide
some of the needed background to understand the current post:</p>
<ul class="simple">
<li><a class="reference internal" href="../../08/26/joint_conditional_and_marginal_probabilities.html#joint-conditional-and-marginal-probabilities"><em>Joint, conditional and marginal probabilities</em></a></li>
<li><a class="reference internal" href="../../09/11/medical_tests_a_first_example_of_bayesian_calculations.html#bayes-medical-tests"><em>Medical tests, a first example of Bayesian calculations</em></a></li>
</ul>
<p>Check those out to get some background, or jump right in. To be concrete, I&#8217;ll
consider the following scenario:</p>
<ul class="simple">
<li>A computer program outputs a random string of <span class="math">\(1\)</span> s and <span class="math">\(0\)</span> s &#8211;
we&#8217;ll use <span class="code docutils literal"><span class="pre">numpy.random.choice</span></span> in Python as our source for data.  For
example, one sample output could be:</li>
</ul>
<div class="math">
\[D = 0000110001\]</div>
<ul class="simple">
<li>The goal will be to infer the probability of a <span class="math">\(0\)</span> that the program is
using to produce <span class="math">\(D\)</span>.  We&#8217;ll use the notation <span class="math">\(p_{0}\)</span> for the
probability of a <span class="math">\(0\)</span>.  Of course this also means that the probability
of a <span class="math">\(1\)</span> must be <span class="math">\(p_{1} = 1 - p_{0}\)</span>.</li>
<li>As discussed above, we will only consider a set of candidate probabilities.
To be concrete, let&#8217;s use the candidates <span class="math">\(p_{0} = 0.2, 0.4, 0.6, 0.8\)</span>
for the data series above. How do we sensibly choose among these
possibilities <strong>and</strong> how certain are we of the result?</li>
</ul>
<div class="section" id="likelihood">
<h2>Likelihood</h2>
<p>Our starting point is to connect the data <span class="math">\(D\)</span> to our assumptions about
the probability of a <span class="math">\(0\)</span> or a <span class="math">\(1\)</span>.  For example, the probability of
<span class="math">\(D\)</span>, without being specific about the value of <span class="math">\(p_{0}\)</span>, can be
written:</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
P(D=0000110001 \vert p_{0} )
    &amp; =      &amp; p_{0} \times p_{0} \times p_{0} \\
    &amp; \times &amp; p_{0} \times (1-p_{0}) \times (1-p_{0}) \\
    &amp; \times &amp; p_{0} \times p_{0} \times p_{0} \\
    &amp; \times &amp; (1-p_{0})
\end{array}\end{split}\]</div>
<p>where we used <span class="math">\(p_{1} = 1 - p_{0}\)</span> to write the probability in terms of
just <span class="math">\(p_{0}\)</span>. We can also collect terms and write the above probability
as:</p>
<div class="math">
\[P(D=0000110001 \vert p_{0} ) = p_{0}^{7} \times (1-p_{0})^{3}\]</div>
<p><strong>Technical note:</strong> the form of the probabilities given above is called a
<a class="reference external" href="http://en.wikipedia.org/wiki/Bernoulli_process">Bernoulli Process</a> (as opposed to a <a class="reference external" href="http://en.wikipedia.org/wiki/Bernoulli_trial">Bernoulli Trial</a> or
<a class="reference external" href="http://en.wikipedia.org/wiki/Binomial_distribution">Binomial Distribution</a>).</p>
<p>Writing the probability of <span class="math">\(D\)</span> this way means that we assume the program
is very simple&#8211; the probability of a <span class="math">\(0\)</span> is always <span class="math">\(p_{0}\)</span>&#8211; and
does not depend on previous numbers or where in the data stream we are
considering.</p>
<p>We can write this probability in a very general way, without being specific
about the data series <span class="math">\(D\)</span> or probability <span class="math">\(p_{0}\)</span>, as:</p>
<div class="math">
\[P(D \vert p_{0}) = p_{0}^{n_{0}} \times (1 - p_{0})^{n_{1}}\]</div>
<p>where <span class="math">\(n_{0}\)</span> and <span class="math">\(n_{1}\)</span> are the number of <span class="math">\(0\)</span> s and
<span class="math">\(1\)</span> s in the data series we are considering. However, we can also be
specific and calculate the likelihood values for the data and probabilities
given above:</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
P(D=0000110001 \vert p_{0}=0.2) &amp; = &amp; 0.2^{7} \times (1-0.2)^{3} \\
                                &amp; = &amp; 6.55360 \times 10^{-6} \\
                                &amp; &amp; \\
P(D=0000110001 \vert p_{0}=0.4) &amp; = &amp; 0.4^{7} \times (1-0.4)^{3} \\
                                &amp; = &amp; 3.53894 \times 10^{-4} \\
                                &amp; &amp; \\
P(D=0000110001 \vert p_{0}=0.6) &amp; = &amp; 0.6^{7} \times (1-0.6)^{3} \\
                                &amp; = &amp; 1.79159 \times 10^{-3} \\
                                &amp; &amp; \\
P(D=0000110001 \vert p_{0}=0.8) &amp; = &amp; 0.8^{7} \times (1-0.8)^{3} \\
                                &amp; = &amp; 1.67772 \times 10^{-3} \\
\end{array}\end{split}\]</div>
<p>From the above, we see that the value that maximizes the likelihood is
<span class="math">\(p_{0}=0.6\)</span>, slightly beating out <span class="math">\(p_{0}=0.8\)</span>.  A couple of things
to note here are:</p>
<ul class="simple">
<li>We have the maximum likelihood value (among the values considered). We could
provide the answer <span class="math">\(p_{0}=0.6\)</span> and be done.</li>
<li>The sum of the probabilities (likelihoods) <strong>is not 1</strong> &#8211; this means that we
do not have a properly normalized <a class="reference external" href="http://en.wikipedia.org/wiki/Probability_mass_function">probability mass function</a> (pmf) with
respect to <span class="math">\(p_{0}\)</span>, the parameter that we are trying to infer. A goal
of Bayesian inference is to provide a properly normalized pmf for
<span class="math">\(p_{0}\)</span>, called the posterior.</li>
</ul>
<p>The ability to do the above calculations puts us in good shape to apply
Bayes&#8217; Theorem and obtain the desired posterior pmf. Before moving on to Bayes&#8217;
Theorem we re-emphasize the general form of the <strong>likelihood</strong>:</p>
<div class="math">
\[P(D \vert p_{0}) = p_{0}^{n_{0}} \times (1 - p_{0})^{n_{1}}\]</div>
<p>It will also be useful to have the <strong>log-likelihood</strong> written down:</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
\ln P(D \vert p_{0}) &amp; = &amp; n_{0} \times \ln(p_{0}) \\
                     &amp; + &amp; n_{1} \times \ln(1 - p_{0})
\end{array}\end{split}\]</div>
<p>because this form adds to the numerical stability when we create some Python
code below. If you are rusty with logarithms, check out
<a class="reference external" href="http://en.wikipedia.org/wiki/Logarithm#Logarithmic_identities">wikipedia logarithm identities</a> for examples of how to get from the
likelihood to the log-likelihood. To be clear, I am using natural (base-e)
logarithms, that is <span class="math">\(\log_{e}(x) = \ln(x)\)</span>.</p>
</div>
<div class="section" id="prior">
<h2>Prior</h2>
<p>We&#8217;ve already decided on part of the prior&#8211; we&#8217;ve done this by choosing
<span class="math">\(p_{0} \in \{ 0.2, 0.4, 0.6, 0.8 \}\)</span> as the set of probabilities that we
will consider.  All that is left is to assign prior probabilities to each
candidate <span class="math">\(p_{0}\)</span> so that we can start with a properly normalized prior
pmf.  Let&#8217;s say that we have no reason to prefer any of the candidates and
make them equally probable, a priori:</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
P(p_{0}=0.2 \vert A1) &amp; =  &amp; 0.25 \\
P(p_{0}=0.4 \vert A1) &amp; =  &amp; 0.25 \\
P(p_{0}=0.6 \vert A1) &amp; =  &amp; 0.25 \\
P(p_{0}=0.8 \vert A1) &amp; =  &amp; 0.25 \\
\end{array}\end{split}\]</div>
<p>where use <span class="math">\(A1\)</span> to denote the assumptions that we&#8217;ve made.  The above
information makes up our <strong>prior</strong> pmf.</p>
</div>
<div class="section" id="bayes-theorem-and-the-posterior">
<h2>Bayes&#8217; Theorem and the Posterior</h2>
<p>Next we employ the <strong>likelihood</strong> and <strong>prior</strong> pmf defined above to make an
inference about the underlying value of <span class="math">\(p_{0}\)</span>. That is, we will use
Bayes&#8217; Theorem to calculate the <strong>posterior</strong> pmf given the likelihood and
prior. The posterior has the form</p>
<div class="math">
\[P(p_{0} \vert D, A1)\]</div>
<p>In words, this is <em>the probability of</em> <span class="math">\(p_{0}\)</span> <em>given data series</em>
<span class="math">\(D\)</span> <em>and assumptions</em> <span class="math">\(A1\)</span>&#8211; hey, that&#8217;s just what we want! We can
calculate the posterior using Bayes&#8217; Theorem:</p>
<div class="math">
\[\color{blue}{P(p_{0} \vert D, A_{1})}
                    =  \frac{
                   P(D \vert p_{0})
                   \color{red}{P(p_{0}\vert A_{1})}
                   }{
                   \sum_{ \hat{p_{0}} }
                   P(D \vert p_{0} = \hat{p_{0}})
                   \color{red}{P(p_{0} = \hat{p_{0}} \vert A_{1})}
                   }\]</div>
<p>where the prior <span class="math">\(\color{red}{P(p_{0} \vert A_{1})}\)</span> is red, the
likelihood <span class="math">\(P(D\vert p_{0})\)</span> is black, and the posterior
<span class="math">\(\color{blue}{P(p_{0} \vert D, A_{1})}\)</span> is blue.  This allows our
information about <span class="math">\(p_{0}\)</span> to updated from <strong>assumptions</strong> (<span class="math">\(A_{1}\)</span>)
to <strong>assumptions + data</strong> (<span class="math">\(D, A_{1}\)</span>):</p>
<div class="math">
\[\color{red}{P(p_{0} \vert A_{1})}
\rightarrow
\color{blue}{P(p_{0} \vert D, A_{1})}\]</div>
<p>We simplify the look of Bayes&#8217; Theorem by defining the <strong>marginal likelihood</strong>,
or <strong>evidence</strong>:</p>
<div class="math">
\[P(D \vert A_{1}) = \sum_{ \hat{p_{0}} }
                   P(D \vert p_{0} = \hat{p_{0}})
                   \color{red}{P(p_{0} = \hat{p_{0}} \vert A_{1})}\]</div>
<p>This lets us write Bayes&#8217; Theorem in the following form:</p>
<div class="math">
\[\color{blue}{P(p_{0} \vert D, A_{1})}
                   =  \frac{
                   P(D \vert p_{0})
                   \color{red}{P(p_{0} \vert A_{1})}
                   }{
                   P(D \vert A_{1})
                   }\]</div>
<p>The posterior should really be thought of as a set of equations, one for each
value of <span class="math">\(p_{0}\)</span>, just like we had for the likelihood and the prior.</p>
<p>Finally, for the theory, we finish off our example and calculate the posterior
pmf for <span class="math">\(p_{0}\)</span>. Let&#8217;s start by calculating the evidence (we know all the
values for the likelihood and prior from above):</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
P(D=0000110001 \vert A_{1})
    &amp; = &amp;
    P(D=0000110001 \vert p_{0} = 0.2) \\
    &amp; \times &amp; P(p_{0} = 0.2 \vert A_{1}) \\
    &amp; + &amp;
    P(D=0000110001 \vert p_{0} = 0.4) \\
    &amp; \times &amp; P(p_{0} = 0.4 \vert A_{1}) \\
    &amp; + &amp;
    P(D=0000110001 \vert p_{0} = 0.6) \\
    &amp; \times &amp; P(p_{0} = 0.6 \vert A_{1}) \\
    &amp; + &amp;
    P(D=0000110001 \vert p_{0} = 0.8) \\
    &amp; \times &amp; P(p_{0} = 0.8 \vert A_{1}) \\
    &amp; = &amp; 6.55360e-06 \times 0.25 \\
    &amp; + &amp; 3.53894e-04 \times 0.25 \\
    &amp; + &amp; 1.79159e-03 \times 0.25 \\
    &amp; + &amp; 1.67772e-03 \times 0.25 \\
    &amp; = &amp; 9.57440e-04
\end{array}\end{split}\]</div>
<p>So, the denominator in Bayes&#8217; Theorem is equal to <span class="math">\(9.57440e-04\)</span>.  Now,
complete the posterior pmf calculation.</p>
<ul class="simple">
<li>First <span class="math">\(P(p_{0} = 0.2 \vert D=0000110001, A_{1})\)</span></li>
</ul>
<div class="math">
\[\begin{split}\begin{array}{ll}
    &amp; = &amp;
    \frac{ P(D=0000110001 \vert p_{0} = 0.2) P(p_{0} = 0.2 \vert A_{1})
    }{ P(D=0000110001 \vert A_{1}) }  \\
    &amp; = &amp; \frac{6.55360e-06 \times 0.25}{9.57440e-04} \\
    &amp; = &amp; 1928309182
\end{array}\end{split}\]</div>
<ul class="simple">
<li>Second, <span class="math">\(P(p_{0} = 0.4 \vert D=0000110001, A_{1})\)</span></li>
</ul>
<div class="math">
\[\begin{split}\begin{array}{ll}
    &amp; = &amp;
    \frac{ P(D=0000110001 \vert p_{0} = 0.4) P(p_{0} = 0.4 \vert A_{1})
    }{ P(D=0000110001 \vert A_{1}) } \\
    &amp; = &amp;
    \frac{ P(D=0000110001 \vert p_{0} = 0.6) P(p_{0} = 0.6 \vert A_{1})
    }{ P(D=0000110001 \vert A_{1}) } \\
    &amp; = &amp;
    \frac{ P(D=0000110001 \vert p_{0} = 0.8) P(p_{0} = 0.8 \vert A_{1})
    }{ P(D=0000110001 \vert A_{1}) } \\
    &amp; = &amp; 6.55360e-06 \times 0.25 \\
    &amp; + &amp; 3.53894e-04 \times 0.25 \\
    &amp; + &amp; 1.79159e-03 \times 0.25 \\
    &amp; + &amp; 1.67772e-03 \times 0.25 \\
    &amp; = &amp; 9.57440e-04
\end{array}\end{split}\]</div>
<p>0.2 6.55360e-06 1.63840e-06
0.4 3.53894e-04 8.84736e-05
0.6 1.79159e-03 4.47898e-04
0.8 1.67772e-03 4.19430e-04
&gt;&gt;&gt; print &#8220;{:1.5e}&#8221;.format(total)
9.57440e-04</p>
</div>
<div class="section" id="writing-the-inference-code-in-python">
<h2>Writing the inference code in Python</h2>
<p>First, some imports</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span><span class="p">,</span> <span class="n">print_function</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="c"># use matplotlib style sheet</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s">&#39;ggplot&#39;</span><span class="p">)</span>
<span class="k">except</span><span class="p">:</span>
    <span class="c"># version of matplotlib might not be recent</span>
    <span class="k">pass</span>
</pre></div>
</div>
<p>Next, a class for inferring probabilities:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">class</span> <span class="nc">likelihood</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Likelihood for binary data.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">counts</span> <span class="o">=</span> <span class="p">{</span><span class="n">s</span><span class="p">:</span><span class="mi">0</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="p">[</span><span class="s">&#39;0&#39;</span><span class="p">,</span> <span class="s">&#39;1&#39;</span><span class="p">]}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_process_data</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_process_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Process data.&quot;&quot;&quot;</span>
        <span class="n">temp</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="p">[</span><span class="s">&#39;0&#39;</span><span class="p">,</span> <span class="s">&#39;1&#39;</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">counts</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">temp</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">counts</span><span class="o">.</span><span class="n">values</span><span class="p">()):</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s">&quot;Passed data is not all 0`s and 1`s!&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_process_probabilities</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Process probabilities.&quot;&quot;&quot;</span>
        <span class="n">n0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">counts</span><span class="p">[</span><span class="s">&#39;0&#39;</span><span class="p">]</span>
        <span class="n">n1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">counts</span><span class="p">[</span><span class="s">&#39;1&#39;</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">p0</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">p0</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c"># typical casee</span>
            <span class="n">logpr_data</span> <span class="o">=</span> <span class="n">n0</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p0</span><span class="p">)</span> <span class="o">+</span> \
                         <span class="n">n1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1.</span><span class="o">-</span><span class="n">p0</span><span class="p">)</span>
            <span class="n">pr_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logpr_data</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">p0</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">n0</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c"># p0 can&#39;t be 0 if n0 is not 0</span>
            <span class="n">logpr_data</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
            <span class="n">pr_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logpr_data</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">p0</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">n0</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c"># data consistent with p0=0</span>
            <span class="n">logpr_data</span> <span class="o">=</span> <span class="n">n1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1.</span><span class="o">-</span><span class="n">p0</span><span class="p">)</span>
            <span class="n">pr_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logpr_data</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">p0</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">n1</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c"># p0 can&#39;t be 1 in n1 is not 0</span>
            <span class="n">logpr_data</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
            <span class="n">pr_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logpr_data</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">p0</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">n1</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c"># data consistent with p0=1</span>
            <span class="n">logpr_data</span> <span class="o">=</span> <span class="n">n0</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p0</span><span class="p">)</span>
            <span class="n">pr_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logpr_data</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">pr_data</span><span class="p">,</span> <span class="n">logpr_data</span>

    <span class="k">def</span> <span class="nf">prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Get probability of data.&quot;&quot;&quot;</span>
        <span class="n">pr_data</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_probabilities</span><span class="p">(</span><span class="n">p0</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">pr_data</span>

    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Get log of probability of data.&quot;&quot;&quot;</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">logpr_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_probabilities</span><span class="p">(</span><span class="n">p0</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">logpr_data</span>
</pre></div>
</div>
<p>Construct the prior class</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">class</span> <span class="nc">prior</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p_list</span><span class="p">,</span> <span class="n">p_probs</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;The prior.</span>

<span class="sd">        p_list: list of allowed p0&#39;s</span>
<span class="sd">        p_probs: [optional] dict of prior probabilities</span>
<span class="sd">                 default is uniform</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">p_probs</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log_pdict</span> <span class="o">=</span> <span class="p">{</span><span class="n">p</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p_probs</span><span class="p">[</span><span class="n">p</span><span class="p">])</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">p_list</span><span class="p">}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">p_list</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log_pdict</span> <span class="o">=</span> <span class="p">{</span><span class="n">p</span><span class="p">:</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">p_list</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">iter</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_pdict</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Get log prior probability for passed p0.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_pdict</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_pdict</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>

    <span class="k">def</span> <span class="nf">prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Get prior probability for passed p0.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_pdict</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_pdict</span><span class="p">[</span><span class="n">p</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="mf">0.0</span>
</pre></div>
</div>
<p>Construct the posterior class:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">class</span> <span class="nc">posterior</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">prior</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;The posterior.</span>

<span class="sd">        data: a data sample as list</span>
<span class="sd">        prior: an instance of the prior class</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span> <span class="o">=</span> <span class="n">likelihood</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prior</span> <span class="o">=</span> <span class="n">prior</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_process_posterior</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_process_posterior</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Process the posterior using passed data and prior.&quot;&quot;&quot;</span>

        <span class="c"># get all numerators in Bayes&#39; Theorem</span>
        <span class="c"># - also keep track of denominator = sum of all numerators</span>
        <span class="n">numerators</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">denominator</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">:</span>
            <span class="n">numerators</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">+</span> \
                            <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">numerators</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="o">!=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">:</span>
                <span class="c"># np.logaddexp(-np.inf, -np.inf) issues warning</span>
                <span class="c"># skip-- this is adding 0 + 0</span>
                <span class="n">denominator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logaddexp</span><span class="p">(</span><span class="n">denominator</span><span class="p">,</span> <span class="n">numerators</span><span class="p">[</span><span class="n">p</span><span class="p">])</span>

        <span class="c"># save denominator in Bayes&#39; Theorm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_marg_likelihood</span> <span class="o">=</span> <span class="n">denominator</span>

        <span class="c"># calculate posterior</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_pdict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log_pdict</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="o">=</span> <span class="n">numerators</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="o">-</span> \
                                <span class="bp">self</span><span class="o">.</span><span class="n">log_marg_likelihood</span>

    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Get log posterior probability for passed p.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_pdict</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_pdict</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>

    <span class="k">def</span> <span class="nf">prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Get posterior probability for passed p.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_pdict</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_pdict</span><span class="p">[</span><span class="n">p</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="mf">0.0</span>

    <span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Plot the inference resuults.&quot;&quot;&quot;</span>

        <span class="n">f</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="c"># get candidate probabilities from prior</span>
        <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">]</span>

        <span class="c"># plot prior</span>
        <span class="n">y1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">x</span><span class="p">])</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">linefmt</span><span class="o">=</span><span class="s">&#39;r-&#39;</span><span class="p">,</span> <span class="n">markerfmt</span><span class="o">=</span><span class="s">&#39;ro&#39;</span><span class="p">,</span> <span class="n">basefmt</span><span class="o">=</span><span class="s">&#39;w-&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">&quot;Prior&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.05</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">y1</span><span class="p">))</span>

        <span class="c"># plot likelihood</span>
        <span class="n">y2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">x</span><span class="p">])</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="n">linefmt</span><span class="o">=</span><span class="s">&#39;k-&#39;</span><span class="p">,</span> <span class="n">markerfmt</span><span class="o">=</span><span class="s">&#39;ko&#39;</span><span class="p">,</span> <span class="n">basefmt</span><span class="o">=</span><span class="s">&#39;w-&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">&quot;Likelihood&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.05</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">y2</span><span class="p">))</span>

        <span class="c"># plot posterior</span>
        <span class="n">y3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">x</span><span class="p">])</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y3</span><span class="p">,</span> <span class="n">linefmt</span><span class="o">=</span><span class="s">&#39;b-&#39;</span><span class="p">,</span> <span class="n">markerfmt</span><span class="o">=</span><span class="s">&#39;bo&#39;</span><span class="p">,</span> <span class="n">basefmt</span><span class="o">=</span><span class="s">&#39;w-&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">&quot;Posterior&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">&quot;Probability of Zero&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.05</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">y3</span><span class="p">))</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Let&#8217;s test out the code... First, replicate the example we did in detail above:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># data</span>
<span class="n">data1</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>

<span class="c"># prior</span>
<span class="c">#A1 = prior([0.2, 0.4, 0.6, 0.8])</span>
<span class="n">A1</span> <span class="o">=</span> <span class="n">prior</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.1</span><span class="p">,</span><span class="mf">0.1</span><span class="p">))</span>

<span class="c"># posterior</span>
<span class="n">post1</span> <span class="o">=</span> <span class="n">posterior</span><span class="p">(</span><span class="n">data1</span><span class="p">,</span> <span class="n">A1</span><span class="p">)</span>
<span class="n">post1</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="../../../_images/inferring_probabilities_a_second_example_of_bayesian_calculations_figure5_1.png"><img alt="../../../_images/inferring_probabilities_a_second_example_of_bayesian_calculations_figure5_1.png" src="../../../_images/inferring_probabilities_a_second_example_of_bayesian_calculations_figure5_1.png" style="width: 15cm;" /></a>
<p>We can also analyze the data above using a different, bad prior &#8211; maybe a
friend that is helping us is an expert and suggest that the probablity is more
likely to be <span class="math">\(p_{0}=0.6\)</span> than the other candidate values. Using our Python
code it is easy to see the effect of this prior:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># prior</span>
<span class="n">A1_friend</span> <span class="o">=</span> <span class="n">prior</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span>
                  <span class="p">{</span><span class="mf">0.2</span><span class="p">:</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">:</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">:</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">:</span><span class="mf">0.2</span><span class="p">})</span>

<span class="c"># posterior</span>
<span class="n">post1_friend</span> <span class="o">=</span> <span class="n">posterior</span><span class="p">(</span><span class="n">data1</span><span class="p">,</span> <span class="n">A1_friend</span><span class="p">)</span>
<span class="n">post1_friend</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="../../../_images/inferring_probabilities_a_second_example_of_bayesian_calculations_figure6_1.png"><img alt="../../../_images/inferring_probabilities_a_second_example_of_bayesian_calculations_figure6_1.png" src="../../../_images/inferring_probabilities_a_second_example_of_bayesian_calculations_figure6_1.png" style="width: 15cm;" /></a>
<p>Next, let&#8217;s consider lots of data and many candidate probabilities:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># set probability of 0</span>
<span class="n">p0</span> <span class="o">=</span> <span class="mf">0.23</span>
<span class="c"># set rng seed to 42, the meaning of like..</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="c"># generate data</span>
<span class="n">data2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="mi">500</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="p">[</span><span class="n">p0</span><span class="p">,</span> <span class="mf">1.</span><span class="o">-</span><span class="n">p0</span><span class="p">])</span>

<span class="c"># prior</span>
<span class="n">A2</span> <span class="o">=</span> <span class="n">prior</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.01</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">))</span>

<span class="c"># posterior</span>
<span class="n">post2</span> <span class="o">=</span> <span class="n">posterior</span><span class="p">(</span><span class="n">data2</span><span class="p">,</span> <span class="n">A2</span><span class="p">)</span>
<span class="n">post2</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="../../../_images/inferring_probabilities_a_second_example_of_bayesian_calculations_figure7_1.png"><img alt="../../../_images/inferring_probabilities_a_second_example_of_bayesian_calculations_figure7_1.png" src="../../../_images/inferring_probabilities_a_second_example_of_bayesian_calculations_figure7_1.png" style="width: 15cm;" /></a>
</div>
</div>

    <div class="postmeta">
        <div class="author">
            <span>Posted by Chris Strelioff</span>
        </div>
        
        <div class="tags">
            <span>
                Tags:
                <a href="../../../tags/joint_probability.html">joint probability</a>, <a href="../../../tags/conditional_probability.html">conditional probability</a>, <a href="../../../tags/marginal_probability.html">marginal probability</a>, <a href="../../../tags/bayesian.html">Bayesian</a>, <a href="../../../tags/python.html">python</a></span>
        </div>
        </div><ul class="related clearfix">
            <li class="left"></li>
            <li class="right"><a href="../../09/16/python_3_4_on_ubuntu_14_04_using_virtual_environments.html">Python 3.4 on Ubuntu 14.04 using virtual environments</a> &raquo; </li>
        </ul><div id="disqus_thread"></div><script type="text/javascript">    var disqus_shortname = "chrissandbox";    var disqus_identifier = "2014/10/24/inferring_probabilities_a_second_example_of_bayesian_calculations";    disqus_thread();</script><noscript>Please enable JavaScript to view the    <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></article><aside class="sidebar"><section><div class="widget" id="searchbox" role="search">
    <h1>Search</h1>
    <form action="../../../search.html" method="get">
        <input type="text" name="q" />
        <button type="submit"><span class="fa fa-search"></span></button>
    </form>
</div></section><section><div class="widget">
  <h1>Pages</h1>
  <ul>
  <li>
      <a href="../../../index.html">Home</a>
    </li>
  </ul>
</div>
</section><section><div class="widget">
    <h1>Recent Posts</h1>
    <ul><li>
            <a href="#">Inferring probabilities, a second example of Bayesian calculations</a>
        </li><li>
            <a href="../../09/16/python_3_4_on_ubuntu_14_04_using_virtual_environments.html">Python 3.4 on Ubuntu 14.04 using virtual environments</a>
        </li><li>
            <a href="../../09/11/medical_tests_a_first_example_of_bayesian_calculations.html">Medical tests, a first example of Bayesian calculations</a>
        </li><li>
            <a href="../../09/04/virtualenv_and_virtualenvwrapper_on_ubuntu_14_04.html">virtualenv and virtualenvwrapper on Ubuntu 14.04</a>
        </li><li>
            <a href="../../09/02/installing_virtualbox_on_ubuntu_14_04.html">Installing virtualbox on Ubuntu 14.04</a>
        </li><li>
            <a href="../../08/26/joint_conditional_and_marginal_probabilities.html">Joint, conditional and marginal probabilities</a>
        </li><li>
            <a href="../../08/16/garmin_forerunner_and_ubuntu_14_04.html">Garmin forerunner and Ubuntu 14.04</a>
        </li><li>
            <a href="../../08/15/install_r_on_ubuntu_14_04.html">Install R on Ubuntu 14.04</a>
        </li><li>
            <a href="../../07/29/install_igraph_for_python_on_ubuntu_14_04.html">Install igraph for Python on Ubuntu 14.04</a>
        </li><li>
            <a href="../../07/12/python_and_yaml_on_ubuntu_14_04.html">Python and YAML on Ubuntu 14.04</a>
        </li></ul>
</div>
</section><section><div class="widget">
    <h1>Tags</h1><a href="../../../tags/bayesian.html">Bayesian</a> (3), <a href="../../../tags/blog_setup.html">blog setup</a> (1), <a href="../../../tags/bootstrap.html">bootstrap</a> (1), <a href="../../../tags/bottleneck.html">bottleneck</a> (1), <a href="../../../tags/caret.html">caret</a> (1), <a href="../../../tags/cmpy.html">cmpy</a> (1), <a href="../../../tags/conditional_probability.html">conditional probability</a> (3), <a href="../../../tags/coursera.html">coursera</a> (1), <a href="../../../tags/coursera_intro_to_data_science.html">coursera intro to data science</a> (3), <a href="../../../tags/cython.html">cython</a> (1), <a href="../../../tags/e1071.html">e1071</a> (1), <a href="../../../tags/garmin.html">garmin</a> (1), <a href="../../../tags/ggplot2.html">ggplot2</a> (1), <a href="../../../tags/git.html">git</a> (1), <a href="../../../tags/gnuplot.html">gnuplot</a> (1), <a href="../../../tags/graphs.html">graphs</a> (1), <a href="../../../tags/igraph.html">igraph</a> (1), <a href="../../../tags/ipython.html">ipython</a> (1), <a href="../../../tags/joint_probability.html">joint probability</a> (3), <a href="../../../tags/latex.html">LaTeX</a> (1), <a href="../../../tags/machine_learning.html">machine learning</a> (1), <a href="../../../tags/marginal_probability.html">marginal probability</a> (3), <a href="../../../tags/matplotlib.html">matplotlib</a> (1), <a href="../../../tags/my_python_setup.html">my python setup</a> (6), <a href="../../../tags/my_ubuntu_setup.html">my ubuntu setup</a> (10), <a href="../../../tags/networks.html">networks</a> (1), <a href="../../../tags/networkx.html">networkx</a> (1), <a href="../../../tags/numexpr.html">numexpr</a> (1), <a href="../../../tags/numpy.html">numpy</a> (1), <a href="../../../tags/octave.html">octave</a> (1), <a href="../../../tags/openpyxl.html">openpyxl</a> (1), <a href="../../../tags/pandas.html">pandas</a> (1), <a href="../../../tags/patsy.html">patsy</a> (1), <a href="../../../tags/pip.html">pip</a> (1), <a href="../../../tags/pweave.html">pweave</a> (1), <a href="../../../tags/pygraphviz.html">pygraphviz</a> (1), <a href="../../../tags/pymc.html">pymc</a> (1), <a href="../../../tags/python.html">python</a> (3), <a href="../../../tags/python_2_7.html">python 2.7</a> (5), <a href="../../../tags/python_3_4.html">python 3.4</a> (1), <a href="../../../tags/pyyaml.html">pyyaml</a> (1), <a href="../../../tags/r.html">R</a> (1), <a href="../../../tags/randomforest.html">randomForest</a> (1), <a href="../../../tags/restview.html">restview</a> (1), <a href="../../../tags/resume.html">resume</a> (1), <a href="../../../tags/rpart.html">rpart</a> (1), <a href="../../../tags/running.html">running</a> (1), <a href="../../../tags/scikit_learn.html">scikit-learn</a> (1), <a href="../../../tags/scipy.html">scipy</a> (1), <a href="../../../tags/screen.html">screen</a> (1), <a href="../../../tags/server_setup.html">server setup</a> (1), <a href="../../../tags/social_networks.html">social networks</a> (1), <a href="../../../tags/sphinx.html">sphinx</a> (1), <a href="../../../tags/sql.html">sql</a> (1), <a href="../../../tags/sqlite3.html">sqlite3</a> (1), <a href="../../../tags/ssh.html">ssh</a> (1), <a href="../../../tags/ssh_keys.html">ssh keys</a> (1), <a href="../../../tags/statsmodels.html">statsmodels</a> (1), <a href="../../../tags/sympy.html">sympy</a> (1), <a href="../../../tags/tableau.html">tableau</a> (1), <a href="../../../tags/tinkerer.html">tinkerer</a> (1), <a href="../../../tags/tree.html">tree</a> (1), <a href="../../../tags/ubuntu_14_04.html">ubuntu 14.04</a> (7), <a href="../../../tags/vim.html">vim</a> (1), <a href="../../../tags/virtualbox.html">virtualbox</a> (1), <a href="../../../tags/virtualenv.html">virtualenv</a> (3), <a href="../../../tags/virtualenvwrapper.html">virtualenvwrapper</a> (2), <a href="../../../tags/vps.html">VPS</a> (1), <a href="../../../tags/yaml.html">yaml</a> (1)</div></section></aside></div> <!-- #main --></div> <!-- #main-container -->

        <div class="footer-container" role="contentinfo"><footer class="wrapper">&copy; Copyright 2014 Christopher Strelioff. Powered by <a href="http://www.tinkerer.me/">Tinkerer</a> and <a href="http://sphinx.pocoo.org/">Sphinx</a>.</footer></div> <!-- footer-container -->

      </div> <!--! end of #container --><!--[if lt IE 7 ]>
          <script src="//ajax.googleapis.com/ajax/libs/chrome-frame/1.0.3/CFInstall.min.js"></script>
          <script>window.attachEvent('onload',function(){CFInstall.check({mode:'overlay'})})</script>
        <![endif]-->
    </body>
</html>