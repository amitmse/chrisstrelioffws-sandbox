<!DOCTYPE html><!--[if lt IE 7]>      <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js"> <!--<![endif]-->
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="description" content="Christopher Strelioff's blog">
        <meta name="viewport" content="width=device-width">
        <title>Inferring probabilities, a second example of Bayesian calculations &mdash; chris&#39; sandbox</title>
            <link rel="stylesheet" href="../../../_static/normalize.css" type="text/css">
            <link rel="stylesheet" href="../../../_static/sphinx.css" type="text/css">
            <link rel="stylesheet" href="../../../_static/main.css" type="text/css">
            <link rel="stylesheet" href="../../../_static/flat.css" type="text/css">
            <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
            <link rel="stylesheet" href="../../../_static/font-awesome.min.css" type="text/css">
        <link rel="stylesheet" href="../../../_static/style.css" type="text/css" /><link rel="shortcut icon" href="../../../_static/tinkerer.ico" /><!-- Load modernizr and JQuery -->
        <script src="../../../_static/vendor/modernizr-2.6.2.min.js"></script>
        <script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.2/jquery.min.js"></script>
        <script>window.jQuery || document.write('<script src="../../../_static/vendor/jquery-1.8.2.min.js"><\/script>')</script>
        <script src="../../../_static/plugins.js"></script>
        <script src="../../../_static/main.js"></script>
        <link rel="next" title="Python 3.4 on Ubuntu 14.04 using virtual environments" href="../../09/16/python_3_4_on_ubuntu_14_04_using_virtual_environments.html" /><link rel="prev" title="Getting started with Latent Dirichlet Allocation in Python" href="../../11/13/getting_started_with_latent_dirichlet_allocation_in_python.html" /><link rel="alternate" type="application/rss+xml" title="RSS" href="../../../rss.html" /><script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     '1.4.2',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script><script type="text/javascript" src="../../../_static/underscore.js"></script><script type="text/javascript" src="../../../_static/doctools.js"></script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript" src="../../../_static/disqus.js"></script><script type="text/javascript" src="../../../_static/google_analytics.js"></script>

    <script type="text/javascript">
        $(document).ready(function () {
            // Scroll to content if on small screen
            if (screen.width < 480)
            {
                $(document).scrollTop(document.getElementsByTagName("article")[0].offsetTop - 44);
            }
        });
    </script></head>
    <body role="document">
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> or <a href="http://www.google.com/chromeframe/?redirect=true">activate Google Chrome Frame</a> to improve your experience.</p>
        <![endif]-->

      <div id="container"><header role="banner">
    <hgroup>
      <h1><a href="../../../index.html">chris&#39; sandbox</a></h1><h2>python, R, ubuntu, bayesian methods, machine learning and more...</h2></hgroup>
  </header>
<div class="main-container" role="main"><div class="main wrapper clearfix"><article><div class="timestamp postmeta">
            <span>October 24, 2014</span>
        </div>
    <div class="section" id="inferring-probabilities-a-second-example-of-bayesian-calculations">
<h1>Inferring probabilities, a second example of Bayesian calculations</h1>
<p>In this post I will focus on an example of inferring probabilities given a
short data series.  I will start by tackling the theory of how
to do the desired inference in a Bayesian way and will end by implementing
the theory in Python so that we can play around with the ideas.  In an attempt
to keep the post more accessible, I will only consider a small set of
candidate probabilities. This restriction allows me to minimize the
mathematical difficulty of the inference and still obtain really cool results,
including nice plots of the <strong>prior</strong>, <strong>likelihood</strong> and <strong>posterior</strong>.</p>
<div id="more"> </div><p>If the content below seems unfamiliar try reading previous posts that provide
some of the needed background to understand the current post:</p>
<ul class="simple">
<li><a class="reference internal" href="../../08/26/joint_conditional_and_marginal_probabilities.html#joint-conditional-and-marginal-probabilities"><em>Joint, conditional and marginal probabilities</em></a></li>
<li><a class="reference internal" href="../../09/11/medical_tests_a_first_example_of_bayesian_calculations.html#bayes-medical-tests"><em>Medical tests, a first example of Bayesian calculations</em></a></li>
</ul>
<p>Check those out to get some background, or jump right in. To be concrete, I&#8217;ll
consider the following scenario:</p>
<ul class="simple">
<li>A computer program outputs a random string of <span class="math">\(1\)</span> s and <span class="math">\(0\)</span> s &#8211;
we&#8217;ll use <span class="code docutils literal"><span class="pre">numpy.random.choice</span></span> in Python as our source for data.  For
example, one sample output could be:</li>
</ul>
<div class="math">
\[D = 0000110001\]</div>
<ul class="simple">
<li>The goal will be to infer the probability of a <span class="math">\(0\)</span> that the program is
using to produce <span class="math">\(D\)</span>.  We&#8217;ll use the notation <span class="math">\(p_{0}\)</span> for the
probability of a <span class="math">\(0\)</span>.  Of course this also means that the probability
of a <span class="math">\(1\)</span> must be <span class="math">\(p_{1} = 1 - p_{0}\)</span>.</li>
<li>As discussed above, we will only consider a set of candidate probabilities.
To be concrete, let&#8217;s use the candidates <span class="math">\(p_{0} = 0.2, 0.4, 0.6, 0.8\)</span>
for the data series above. How do we sensibly choose among these
possibilities <strong>and</strong> how certain are we of the result?</li>
</ul>
<div class="section" id="likelihood">
<h2>Likelihood</h2>
<p>My starting point is to write down the probability of the data series <em>as if I
knew the probability of a</em> <span class="math">\(0\)</span> <em>or a</em> <span class="math">\(1\)</span>. Of course I don&#8217;t know
these probabilities&#8211; finding these probabilities is our goal&#8211; but trust me,
this is leading somewhere useful. For example, the probability of our example
data series, without being specific about the value of <span class="math">\(p_{0}\)</span>, can be
written:</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
P(D=0000110001 \vert p_{0} )
    &amp; =      &amp; p_{0} \times p_{0} \times p_{0} \\
    &amp; \times &amp; p_{0} \times (1-p_{0}) \times (1-p_{0}) \\
    &amp; \times &amp; p_{0} \times p_{0} \times p_{0} \\
    &amp; \times &amp; (1-p_{0})
\end{array}\end{split}\]</div>
<p>where I&#8217;ve used <span class="math">\(p_{1} = 1 - p_{0}\)</span> to write the probability in terms of
just <span class="math">\(p_{0}\)</span>. I can also collect terms and write the above probability
in a more compact way:</p>
<div class="math">
\[P(D=0000110001 \vert p_{0} ) = p_{0}^{7} \times (1-p_{0})^{3}\]</div>
<p><strong>Technical aside:</strong> the form of the probabilities given above is called a
<a class="reference external" href="http://en.wikipedia.org/wiki/Bernoulli_process">Bernoulli Process</a> (as opposed to a <a class="reference external" href="http://en.wikipedia.org/wiki/Bernoulli_trial">Bernoulli Trial</a> or
<a class="reference external" href="http://en.wikipedia.org/wiki/Binomial_distribution">Binomial Distribution</a>). I can also write this probability in a very general
way, without being specific about the data series <span class="math">\(D\)</span> or probability
<span class="math">\(p_{0}\)</span>, as:</p>
<div class="math">
\[P(D \vert p_{0}) = p_{0}^{n_{0}} \times (1 - p_{0})^{n_{1}}\]</div>
<p><span class="math">\(n_{0}\)</span> and <span class="math">\(n_{1}\)</span> denote the number of <span class="math">\(0\)</span> s and
<span class="math">\(1\)</span> s in the data series I am considering.</p>
<p>I can connect the general form to a specific example by substituting the
relevant counts and probabilities.  I&#8217;ll start by calculating the likelihood
values for the data series and probabilities given above:</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
P(D=0000110001 \vert p_{0}=0.2) &amp; = &amp; 0.2^{7} \times (1-0.2)^{3} \\
                                &amp; = &amp; 6.55360 \times 10^{-6} \\
                                &amp; &amp; \\
P(D=0000110001 \vert p_{0}=0.4) &amp; = &amp; 0.4^{7} \times (1-0.4)^{3} \\
                                &amp; = &amp; 3.53894 \times 10^{-4} \\
                                &amp; &amp; \\
P(D=0000110001 \vert p_{0}=0.6) &amp; = &amp; 0.6^{7} \times (1-0.6)^{3} \\
                                &amp; = &amp; 1.79159 \times 10^{-3} \\
                                &amp; &amp; \\
P(D=0000110001 \vert p_{0}=0.8) &amp; = &amp; 0.8^{7} \times (1-0.8)^{3} \\
                                &amp; = &amp; 1.67772 \times 10^{-3} \\
\end{array}\end{split}\]</div>
<p>Inspecting the results, I see that <span class="math">\(p_{0}=0.6\)</span> produces the highest
likelihood, slightly beating out <span class="math">\(p_{0}=0.8\)</span>.  A couple of things
to note here are:</p>
<ul class="simple">
<li>I have the maximum likelihood value (among the values considered). I could
provide the answer <span class="math">\(p_{0}=0.6\)</span> and be done.</li>
<li>The sum of the probabilities (likelihoods) <strong>is not 1</strong> &#8211; this means that I
do not have a properly normalized <a class="reference external" href="http://en.wikipedia.org/wiki/Probability_mass_function">probability mass function</a> (pmf) with
respect to <span class="math">\(p_{0}\)</span>, the parameter that I am trying to infer. A goal
of Bayesian inference is to provide a properly normalized pmf for
<span class="math">\(p_{0}\)</span>, called the posterior.</li>
</ul>
<p>The ability to do the above calculations puts me in good shape to apply
Bayes&#8217; Theorem and obtain the desired posterior pmf. Before moving on to Bayes&#8217;
Theorem I want to re-emphasize the general form of the <strong>likelihood</strong>:</p>
<div class="math">
\[P(D \vert p_{0}) = p_{0}^{n_{0}} \times (1 - p_{0})^{n_{1}}\]</div>
<p>It will also be useful to have the <strong>log-likelihood</strong> written down:</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
\ln P(D \vert p_{0}) &amp; = &amp; n_{0} \times \ln(p_{0}) \\
                     &amp; + &amp; n_{1} \times \ln(1 - p_{0})
\end{array}\end{split}\]</div>
<p>because this form adds to the numerical stability when I create some Python
code below. If you are rusty with logarithms, check out
<a class="reference external" href="http://en.wikipedia.org/wiki/Logarithm#Logarithmic_identities">wikipedia logarithm identities</a> for examples of how to get from the
likelihood to the log-likelihood. To be clear, I am using natural (base-e)
logarithms, that is <span class="math">\(\log_{e}(x) = \ln(x)\)</span>.</p>
</div>
<div class="section" id="prior">
<h2>Prior</h2>
<p>I&#8217;ve already decided on part of the prior&#8211; I&#8217;ve done this by choosing
<span class="math">\(p_{0} \in \{ 0.2, 0.4, 0.6, 0.8 \}\)</span> as the set of probabilities that I
will consider.  All that is left is to assign prior probabilities to each
candidate <span class="math">\(p_{0}\)</span> so that I can start with a properly normalized prior
pmf.  Let&#8217;s say that I have no reason to prefer any of the candidates and
make them equally probable, a priori:</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
P(p_{0}=0.2 \vert A1) &amp; =  &amp; 0.25 \\
P(p_{0}=0.4 \vert A1) &amp; =  &amp; 0.25 \\
P(p_{0}=0.6 \vert A1) &amp; =  &amp; 0.25 \\
P(p_{0}=0.8 \vert A1) &amp; =  &amp; 0.25 \\
\end{array}\end{split}\]</div>
<p>where use <span class="math">\(A1\)</span> to denote the assumptions that I&#8217;ve made.  The above
information makes up my <strong>prior</strong> pmf.</p>
</div>
<div class="section" id="bayes-theorem-and-the-posterior">
<h2>Bayes&#8217; Theorem and the Posterior</h2>
<p>Next I employ the <strong>likelihood</strong> and <strong>prior</strong> pmf defined above to make an
inference about the underlying value of <span class="math">\(p_{0}\)</span>. That is, I will use
Bayes&#8217; Theorem to calculate the <strong>posterior</strong> pmf given the likelihood and
prior. The posterior has the form</p>
<div class="math">
\[P(p_{0} \vert D, A1)\]</div>
<p>In words, this is <em>the probability of</em> <span class="math">\(p_{0}\)</span> <em>given data series</em>
<span class="math">\(D\)</span> <em>and assumptions</em> <span class="math">\(A1\)</span>&#8211; hey, that&#8217;s just what I want! I can
calculate the posterior using Bayes&#8217; Theorem:</p>
<div class="math">
\[\color{blue}{P(p_{0} \vert D, A_{1})}
                    =  \frac{
                   P(D \vert p_{0})
                   \color{red}{P(p_{0}\vert A_{1})}
                   }{
                   \sum_{ \hat{p_{0}} }
                   P(D \vert p_{0} = \hat{p_{0}})
                   \color{red}{P(p_{0} = \hat{p_{0}} \vert A_{1})}
                   }\]</div>
<p>where the prior <span class="math">\(\color{red}{P(p_{0} \vert A_{1})}\)</span> is red, the
likelihood <span class="math">\(P(D\vert p_{0})\)</span> is black, and the posterior
<span class="math">\(\color{blue}{P(p_{0} \vert D, A_{1})}\)</span> is blue.  This allows my
information about <span class="math">\(p_{0}\)</span> to updated from <strong>assumptions</strong> (<span class="math">\(A_{1}\)</span>)
to <strong>assumptions + data</strong> (<span class="math">\(D, A_{1}\)</span>):</p>
<div class="math">
\[\color{red}{P(p_{0} \vert A_{1})}
\rightarrow
\color{blue}{P(p_{0} \vert D, A_{1})}\]</div>
<p>I can simplify the look of Bayes&#8217; Theorem by defining the <strong>marginal
likelihood</strong>, or <strong>evidence</strong>:</p>
<div class="math">
\[P(D \vert A_{1}) = \sum_{ \hat{p_{0}} }
                   P(D \vert p_{0} = \hat{p_{0}})
                   \color{red}{P(p_{0} = \hat{p_{0}} \vert A_{1})}\]</div>
<p>This lets me write Bayes&#8217; Theorem in the following form:</p>
<div class="math">
\[\color{blue}{P(p_{0} \vert D, A_{1})}
                   =  \frac{
                   P(D \vert p_{0})
                   \color{red}{P(p_{0} \vert A_{1})}
                   }{
                   P(D \vert A_{1})
                   }\]</div>
<p>The posterior should really be thought of as a set of equations, one for each
candidate value of <span class="math">\(p_{0}\)</span>, just like we had for the likelihood and the
prior.</p>
<p>Finally, for the theory, I finish off our example and calculate the posterior
pmf for <span class="math">\(p_{0}\)</span>. Let&#8217;s start by calculating the evidence (I know all the
values for the likelihood and prior from above):</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
P(D=0000110001 \vert A_{1})
    &amp; = &amp;
    P(D=0000110001 \vert p_{0} = 0.2) \\
    &amp; \times &amp; P(p_{0} = 0.2 \vert A_{1}) \\
    &amp; + &amp;
    P(D=0000110001 \vert p_{0} = 0.4) \\
    &amp; \times &amp; P(p_{0} = 0.4 \vert A_{1}) \\
    &amp; + &amp;
    P(D=0000110001 \vert p_{0} = 0.6) \\
    &amp; \times &amp; P(p_{0} = 0.6 \vert A_{1}) \\
    &amp; + &amp;
    P(D=0000110001 \vert p_{0} = 0.8) \\
    &amp; \times &amp; P(p_{0} = 0.8 \vert A_{1}) \\
    &amp; = &amp; 6.55360e-06 \times 0.25 \\
    &amp; + &amp; 3.53894e-04 \times 0.25 \\
    &amp; + &amp; 1.79159e-03 \times 0.25 \\
    &amp; + &amp; 1.67772e-03 \times 0.25 \\
    &amp; = &amp; 9.57440e-04
\end{array}\end{split}\]</div>
<p>So, the denominator in Bayes&#8217; Theorem is equal to <span class="math">\(9.57440e-04\)</span>.  Now,
complete the posterior pmf calculation.</p>
<ul class="simple">
<li>First, <span class="math">\(P(p_{0} = 0.2 \vert D=0000110001, A_{1})\)</span></li>
</ul>
<div class="math">
\[\begin{split}\begin{array}{ll}
    &amp; = &amp;
    \frac{ P(D=0000110001 \vert p_{0} = 0.2) P(p_{0} = 0.2 \vert A_{1})
    }{ P(D=0000110001 \vert A_{1}) }  \\
    &amp; = &amp; \frac{6.55360e-06 \times 0.25}{9.57440e-04} \\
    &amp; = &amp; 1.78253e-03
\end{array}\end{split}\]</div>
<ul class="simple">
<li>Second, <span class="math">\(P(p_{0} = 0.4 \vert D=0000110001, A_{1})\)</span></li>
</ul>
<div class="math">
\[\begin{split}\begin{array}{ll}
    &amp; = &amp;
    \frac{ P(D=0000110001 \vert p_{0} = 0.4) P(p_{0} = 0.4 \vert A_{1})
    }{ P(D=0000110001 \vert A_{1}) } \\
    &amp; = &amp; \frac{3.53894e-04 \times 0.25}{9.57440e-04} \\
    &amp; = &amp; 9.62567e-02
\end{array}\end{split}\]</div>
<ul class="simple">
<li>Third, <span class="math">\(P(p_{0} = 0.6 \vert D=0000110001, A_{1})\)</span></li>
</ul>
<div class="math">
\[\begin{split}\begin{array}{ll}
    &amp; = &amp;
    \frac{ P(D=0000110001 \vert p_{0} = 0.6) P(p_{0} = 0.6 \vert A_{1})
    }{ P(D=0000110001 \vert A_{1}) } \\
    &amp; = &amp; \frac{1.79159e-03 \times 0.25}{9.57440e-04} \\
    &amp; = &amp; 4.87299e-01
\end{array}\end{split}\]</div>
<ul class="simple">
<li>Finally, <span class="math">\(P(p_{0} = 0.8 \vert D=0000110001, A_{1})\)</span></li>
</ul>
<div class="math">
\[\begin{split}\begin{array}{ll}
    &amp; = &amp;
    \frac{ P(D=0000110001 \vert p_{0} = 0.8) P(p_{0} = 0.8 \vert A_{1})
    }{ P(D=0000110001 \vert A_{1}) } \\
    &amp; = &amp; \frac{1.67772e-03 \times 0.25}{9.57440e-04} \\
    &amp; = &amp; 4.56328e-01
\end{array}\end{split}\]</div>
</div>
<div class="section" id="summing-up">
<h2>Summing Up</h2>
<p>Before moving on to the Python code, let&#8217;s go over the results a bit. Using the
data series and Bayes&#8217; Theorem I&#8217;ve gone from the <strong>prior</strong> pmf</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
P(p_{0}=0.2 \vert A1) &amp; =  &amp; 0.25 \\
P(p_{0}=0.4 \vert A1) &amp; =  &amp; 0.25 \\
P(p_{0}=0.6 \vert A1) &amp; =  &amp; 0.25 \\
P(p_{0}=0.8 \vert A1) &amp; =  &amp; 0.25 \\
\end{array}\end{split}\]</div>
<p>to the <strong>posterior</strong> pmf (I&#8217;ll shorten the data series in the notation below)</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
P(p_{0}=0.2 \vert D=000\ldots, A1) &amp; =  &amp; 1.78253e-03 \\
P(p_{0}=0.4 \vert D=000\ldots, A1) &amp; =  &amp; 9.62567e-02 \\
P(p_{0}=0.6 \vert D=000\ldots, A1) &amp; =  &amp; 4.87299e-01 \\
P(p_{0}=0.8 \vert D=000\ldots, A1) &amp; =  &amp; 4.56328e-01
\end{array}\end{split}\]</div>
<p>In a Bayesian setting, this posterior pmf is the answer to our inference of
<span class="math">\(p_{0}\)</span>, reflecting our knowledge of the parameter given the assumptions
and data.  Often people want to report a single number but this posterior
reflects a fair amount of uncertainty.  Some options are:</p>
<ul class="simple">
<li>Report the <em>maximum a posteriori</em> value of <span class="math">\(p_{0}\)</span>&#8211; in this case
<span class="math">\(0.6\)</span>.</li>
<li>Report the <em>posterior mean</em>, the <em>posterior median</em> &#8211; using the posterior
pmf to calculate.</li>
<li>Include a posterior variance or credible interval to describe uncertainty in
the estimate.</li>
</ul>
<p>However the inference is reported, communicating the uncertainty is part of the
job.  In practice, plots of the posterior really help with the task.  So, let&#8217;s
leave theory and implement these ideas in Python.</p>
</div>
<div class="section" id="writing-the-inference-code-in-python">
<h2>Writing the inference code in Python</h2>
<p>This code will be available as a single Python script, <span class="code docutils literal"><span class="pre">ex001_bayes.py</span></span>,
at a <a class="reference external" href="https://github.com/cstrelioff/chrisstrelioffws-sandbox-examples">github examples repository</a> I&#8217;ve setup to host such things.  You should
grab it and try to following along.</p>
<p>First, the code has some imports &#8211; just <span class="code docutils literal"><span class="pre">numpy</span></span> and <span class="code docutils literal"><span class="pre">matplotlib</span></span>.
I will also use a nice <span class="code docutils literal"><span class="pre">ggplot</span></span> style to make the plots look really
nice.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span><span class="p">,</span> <span class="n">print_function</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="c"># use matplotlib style sheet</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s">&#39;ggplot&#39;</span><span class="p">)</span>
<span class="k">except</span><span class="p">:</span>
    <span class="c"># version of matplotlib might not be recent</span>
    <span class="k">pass</span>
</pre></div>
</div>
<p>First, I make a class to deal with the <strong>likelihood</strong>.  The class takes the
data series and provides an interface for computing the likelihood for a given
probability <span class="math">\(p_{0}\)</span>.  You should be able to find the <strong>log-likelihood</strong>
equation in the <span class="code docutils literal"><span class="pre">_process_probabilities()</span></span> method (with some care taken
for edge cases).</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">class</span> <span class="nc">likelihood</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Likelihood for binary data.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">counts</span> <span class="o">=</span> <span class="p">{</span><span class="n">s</span><span class="p">:</span><span class="mi">0</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="p">[</span><span class="s">&#39;0&#39;</span><span class="p">,</span> <span class="s">&#39;1&#39;</span><span class="p">]}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_process_data</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_process_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Process data.&quot;&quot;&quot;</span>
        <span class="n">temp</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="p">[</span><span class="s">&#39;0&#39;</span><span class="p">,</span> <span class="s">&#39;1&#39;</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">counts</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">temp</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">counts</span><span class="o">.</span><span class="n">values</span><span class="p">()):</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s">&quot;Passed data is not all 0`s and 1`s!&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_process_probabilities</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Process probabilities.&quot;&quot;&quot;</span>
        <span class="n">n0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">counts</span><span class="p">[</span><span class="s">&#39;0&#39;</span><span class="p">]</span>
        <span class="n">n1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">counts</span><span class="p">[</span><span class="s">&#39;1&#39;</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">p0</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">p0</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c"># typical case</span>
            <span class="n">logpr_data</span> <span class="o">=</span> <span class="n">n0</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p0</span><span class="p">)</span> <span class="o">+</span> \
                         <span class="n">n1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1.</span><span class="o">-</span><span class="n">p0</span><span class="p">)</span>
            <span class="n">pr_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logpr_data</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">p0</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">n0</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c"># p0 can&#39;t be 0 if n0 is not 0</span>
            <span class="n">logpr_data</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
            <span class="n">pr_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logpr_data</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">p0</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">n0</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c"># data consistent with p0=0</span>
            <span class="n">logpr_data</span> <span class="o">=</span> <span class="n">n1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1.</span><span class="o">-</span><span class="n">p0</span><span class="p">)</span>
            <span class="n">pr_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logpr_data</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">p0</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">n1</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c"># p0 can&#39;t be 1 if n1 is not 0</span>
            <span class="n">logpr_data</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
            <span class="n">pr_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logpr_data</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">p0</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">n1</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c"># data consistent with p0=1</span>
            <span class="n">logpr_data</span> <span class="o">=</span> <span class="n">n0</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p0</span><span class="p">)</span>
            <span class="n">pr_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logpr_data</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">pr_data</span><span class="p">,</span> <span class="n">logpr_data</span>

    <span class="k">def</span> <span class="nf">prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Get probability of data.&quot;&quot;&quot;</span>
        <span class="n">pr_data</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_probabilities</span><span class="p">(</span><span class="n">p0</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">pr_data</span>

    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Get log of probability of data.&quot;&quot;&quot;</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">logpr_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_probabilities</span><span class="p">(</span><span class="n">p0</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">logpr_data</span>
</pre></div>
</div>
<p>Next I create a class for the <strong>prior</strong> pmf.  Given a list of candidate values
for <span class="math">\(p_{0}\)</span>, this creates a uniform prior by default.  If something
else is desired, a dictionary of prior probabilities can be passed to override
this default.  I&#8217;ll do an example below.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">class</span> <span class="nc">prior</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p_list</span><span class="p">,</span> <span class="n">p_probs</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;The prior.</span>

<span class="sd">        p_list: list of allowed p0&#39;s</span>
<span class="sd">        p_probs: [optional] dict of prior probabilities</span>
<span class="sd">                 default is uniform</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">p_probs</span><span class="p">:</span>
            <span class="c"># make sure prior is normalized</span>
            <span class="n">norm</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p_probs</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log_pdict</span> <span class="o">=</span> <span class="p">{</span><span class="n">p</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p_probs</span><span class="p">[</span><span class="n">p</span><span class="p">])</span> <span class="o">-</span> \
                                <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">norm</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">p_list</span><span class="p">}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">p_list</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log_pdict</span> <span class="o">=</span> <span class="p">{</span><span class="n">p</span><span class="p">:</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">p_list</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">iter</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_pdict</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Get log prior probability for passed p0.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_pdict</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_pdict</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>

    <span class="k">def</span> <span class="nf">prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Get prior probability for passed p0.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_pdict</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_pdict</span><span class="p">[</span><span class="n">p</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="mf">0.0</span>
</pre></div>
</div>
<p>Finally I construct a class for the <strong>posterior</strong> that takes the data series
and an instance of the <span class="code docutils literal"><span class="pre">prior</span></span> class and constructs the posterior pmf.  A
<span class="code docutils literal"><span class="pre">plot()</span></span> method provides a really nice visualization of the inference,
including plots of the <strong>prior</strong>, <strong>likelihood</strong> and <strong>posterior</strong>.</p>
<p>Notice that all of the calculations for the posterior are done using
log-probabilities.  This is absolutely necessary for numerical accuracy because
the probabilities can vary greatly and some are extremely small.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">class</span> <span class="nc">posterior</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">prior</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;The posterior.</span>

<span class="sd">        data: a data sample as list</span>
<span class="sd">        prior: an instance of the prior class</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span> <span class="o">=</span> <span class="n">likelihood</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prior</span> <span class="o">=</span> <span class="n">prior</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_process_posterior</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_process_posterior</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Process the posterior using passed data and prior.&quot;&quot;&quot;</span>

        <span class="n">numerators</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">denominator</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">:</span>
            <span class="n">numerators</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">+</span> \
                            <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">numerators</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="o">!=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">:</span>
                <span class="c"># np.logaddexp(-np.inf, -np.inf) issues warning</span>
                <span class="c"># skip-- this is adding 0 + 0</span>
                <span class="n">denominator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logaddexp</span><span class="p">(</span><span class="n">denominator</span><span class="p">,</span>
                                           <span class="n">numerators</span><span class="p">[</span><span class="n">p</span><span class="p">])</span>

        <span class="c"># save denominator in Bayes&#39; Theorem</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_marg_likelihood</span> <span class="o">=</span> <span class="n">denominator</span>

        <span class="c"># calculate posterior</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_pdict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log_pdict</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="o">=</span> <span class="n">numerators</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="o">-</span> \
                                <span class="bp">self</span><span class="o">.</span><span class="n">log_marg_likelihood</span>

    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Get log posterior probability for passed p.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_pdict</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_pdict</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>

    <span class="k">def</span> <span class="nf">prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Get posterior probability for passed p.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_pdict</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_pdict</span><span class="p">[</span><span class="n">p</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="mf">0.0</span>

    <span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Plot the inference resuults.&quot;&quot;&quot;</span>

        <span class="n">f</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="c"># get candidate probabilities from prior</span>
        <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">]</span>

        <span class="c"># plot prior</span>
        <span class="n">y1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">x</span><span class="p">])</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">linefmt</span><span class="o">=</span><span class="s">&#39;r-&#39;</span><span class="p">,</span> <span class="n">markerfmt</span><span class="o">=</span><span class="s">&#39;ro&#39;</span><span class="p">,</span> <span class="n">basefmt</span><span class="o">=</span><span class="s">&#39;w-&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">&quot;Prior&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.05</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">y1</span><span class="p">))</span>

        <span class="c"># plot likelihood</span>
        <span class="n">y2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">x</span><span class="p">])</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="n">linefmt</span><span class="o">=</span><span class="s">&#39;k-&#39;</span><span class="p">,</span> <span class="n">markerfmt</span><span class="o">=</span><span class="s">&#39;ko&#39;</span><span class="p">,</span> <span class="n">basefmt</span><span class="o">=</span><span class="s">&#39;w-&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">&quot;Likelihood&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.05</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">y2</span><span class="p">))</span>

        <span class="c"># plot posterior</span>
        <span class="n">y3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">x</span><span class="p">])</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y3</span><span class="p">,</span> <span class="n">linefmt</span><span class="o">=</span><span class="s">&#39;b-&#39;</span><span class="p">,</span> <span class="n">markerfmt</span><span class="o">=</span><span class="s">&#39;bo&#39;</span><span class="p">,</span> <span class="n">basefmt</span><span class="o">=</span><span class="s">&#39;w-&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">&quot;Posterior&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">&quot;Probability of Zero&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.05</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">y3</span><span class="p">))</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="examples">
<h2>Examples</h2>
<p>Let&#8217;s test out the code. First, I will replicate the example we did in the
theory example to make sure all is well:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># data</span>
<span class="n">data1</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>

<span class="c"># prior</span>
<span class="n">A1</span> <span class="o">=</span> <span class="n">prior</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">])</span>

<span class="c"># posterior</span>
<span class="n">post1</span> <span class="o">=</span> <span class="n">posterior</span><span class="p">(</span><span class="n">data1</span><span class="p">,</span> <span class="n">A1</span><span class="p">)</span>
<span class="n">post1</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="../../../_images/inferring_probabilities_a_second_example_of_bayesian_calculations_figure5_1.png"><img alt="../../../_images/inferring_probabilities_a_second_example_of_bayesian_calculations_figure5_1.png" src="../../../_images/inferring_probabilities_a_second_example_of_bayesian_calculations_figure5_1.png" style="width: 15cm;" /></a>
<p>Notice how the posterior pmf nicely shows that both <span class="math">\(p_{0}=0.6\)</span> and
<span class="math">\(p_{0}=0.8\)</span> have substantial probability&#8211; there is uncertainty here!
That makes sense because we only have a data series of length 10 and the are
only four candidate probabilities.  Also, notice:</p>
<ul class="simple">
<li>The sums of all stems in the prior and the posterior sum to 1, reflecting
that these are proper pmfs.</li>
<li>The likelihood does not have this property &#8211; look at the scale on the
y-axis.  This gets even worse when we consider a longer data series below.</li>
<li>Because the prior was uniform, the posterior shape looks just like the
likelihood.</li>
</ul>
<p>Next, let&#8217;s consider setting a strong prior &#8211; preferring one value of
<span class="math">\(p_{0}\)</span>.  Using our Python code it is easy to see the effect of this
prior on the resulting posterior:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># prior -- will be normalized by class</span>
<span class="n">A2</span> <span class="o">=</span> <span class="n">prior</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span>
           <span class="p">{</span><span class="mf">0.2</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">:</span><span class="mi">20</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">:</span><span class="mi">1</span><span class="p">})</span>

<span class="c"># posterior</span>
<span class="n">post2</span> <span class="o">=</span> <span class="n">posterior</span><span class="p">(</span><span class="n">data1</span><span class="p">,</span> <span class="n">A2</span><span class="p">)</span>
<span class="n">post2</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="../../../_images/inferring_probabilities_a_second_example_of_bayesian_calculations_figure6_1.png"><img alt="../../../_images/inferring_probabilities_a_second_example_of_bayesian_calculations_figure6_1.png" src="../../../_images/inferring_probabilities_a_second_example_of_bayesian_calculations_figure6_1.png" style="width: 15cm;" /></a>
<p>Notice the following things:</p>
<ul class="simple">
<li>The posterior and the likelihood no longer have the same shape.  The strong
prior affects of inference &#8211; we should have a really good reason to use this
prior!</li>
<li>The posterior probabilities of <span class="math">\(p_{0}=0.2,0.4\)</span> have both <em>decreased
relative to their prior probabilities</em> because of their low likelihood for
the provided data series. In a similar manner, the posterior probabilities
of <span class="math">\(p_{0}=0.6, 0.8\)</span> have <em>increased relative to their prior
probabilities</em>.  This makes sense because of the prior and the data provided!</li>
</ul>
<p>Finally, let&#8217;s do a quick example with more candidate probabilities, 100 in this
case, and a longer data series.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># set probability of 0</span>
<span class="n">p0</span> <span class="o">=</span> <span class="mf">0.23</span>
<span class="c"># set rng seed to 42</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="c"># generate data</span>
<span class="n">data2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="mi">500</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="p">[</span><span class="n">p0</span><span class="p">,</span> <span class="mf">1.</span><span class="o">-</span><span class="n">p0</span><span class="p">])</span>

<span class="c"># prior</span>
<span class="n">A3</span> <span class="o">=</span> <span class="n">prior</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.01</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">))</span>

<span class="c"># posterior</span>
<span class="n">post3</span> <span class="o">=</span> <span class="n">posterior</span><span class="p">(</span><span class="n">data2</span><span class="p">,</span> <span class="n">A3</span><span class="p">)</span>
<span class="n">post3</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="../../../_images/inferring_probabilities_a_second_example_of_bayesian_calculations_figure7_1.png"><img alt="../../../_images/inferring_probabilities_a_second_example_of_bayesian_calculations_figure7_1.png" src="../../../_images/inferring_probabilities_a_second_example_of_bayesian_calculations_figure7_1.png" style="width: 15cm;" /></a>
<p>Notice a few things:</p>
<ul class="simple">
<li>The posterior has a nice smooth shape&#8211; this looks like I treated the
probability as a continuous value (I&#8217;ll do that in a future post).</li>
<li>Notice how small the likelihood values are (y-axis) for this amount of data.
Longer data series will cause <span class="code docutils literal"><span class="pre">matplotlib</span></span> to have trouble plotting.</li>
</ul>
<p>Well, that&#8217;s it.  I hope you find this interesting.  As always, leave
questions, comments and corrections!</p>
</div>
</div>

    <div class="postmeta">
        <div class="author">
            <span>Posted by Chris Strelioff</span>
        </div>
        
        <div class="tags">
            <span>
                Tags:
                <a href="../../../tags/joint_probability.html">joint probability</a>, <a href="../../../tags/conditional_probability.html">conditional probability</a>, <a href="../../../tags/marginal_probability.html">marginal probability</a>, <a href="../../../tags/bayesian.html">Bayesian</a>, <a href="../../../tags/python.html">python</a></span>
        </div>
        </div><ul class="related clearfix">
            <li class="left"> &laquo; <a href="../../11/13/getting_started_with_latent_dirichlet_allocation_in_python.html">Getting started with Latent Dirichlet Allocation in Python</a></li>
            <li class="right"><a href="../../09/16/python_3_4_on_ubuntu_14_04_using_virtual_environments.html">Python 3.4 on Ubuntu 14.04 using virtual environments</a> &raquo; </li>
        </ul><div id="disqus_thread"></div><script type="text/javascript">    var disqus_shortname = "chrissandbox";    var disqus_identifier = "2014/10/24/inferring_probabilities_a_second_example_of_bayesian_calculations";    disqus_thread();</script><noscript>Please enable JavaScript to view the    <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></article><aside class="sidebar"><section><div class="widget" id="searchbox" role="search">
    <h1>Search</h1>
    <form action="../../../search.html" method="get">
        <input type="text" name="q" />
        <button type="submit"><span class="fa fa-search"></span></button>
    </form>
</div></section><section><div class="widget">
  <h1>Pages</h1>
  <ul>
  <li>
      <a href="../../../index.html">Home</a>
    </li>
  </ul>
</div>
</section><section><div class="widget">
    <h1>Recent Posts</h1>
    <ul><li>
            <a href="../../11/13/getting_started_with_latent_dirichlet_allocation_in_python.html">Getting started with Latent Dirichlet Allocation in Python</a>
        </li><li>
            <a href="#">Inferring probabilities, a second example of Bayesian calculations</a>
        </li><li>
            <a href="../../09/16/python_3_4_on_ubuntu_14_04_using_virtual_environments.html">Python 3.4 on Ubuntu 14.04 using virtual environments</a>
        </li><li>
            <a href="../../09/11/medical_tests_a_first_example_of_bayesian_calculations.html">Medical tests, a first example of Bayesian calculations</a>
        </li><li>
            <a href="../../09/04/virtualenv_and_virtualenvwrapper_on_ubuntu_14_04.html">virtualenv and virtualenvwrapper on Ubuntu 14.04</a>
        </li><li>
            <a href="../../09/02/installing_virtualbox_on_ubuntu_14_04.html">Installing virtualbox on Ubuntu 14.04</a>
        </li><li>
            <a href="../../08/26/joint_conditional_and_marginal_probabilities.html">Joint, conditional and marginal probabilities</a>
        </li><li>
            <a href="../../08/16/garmin_forerunner_and_ubuntu_14_04.html">Garmin forerunner and Ubuntu 14.04</a>
        </li><li>
            <a href="../../08/15/install_r_on_ubuntu_14_04.html">Install R on Ubuntu 14.04</a>
        </li><li>
            <a href="../../07/29/install_igraph_for_python_on_ubuntu_14_04.html">Install igraph for Python on Ubuntu 14.04</a>
        </li></ul>
</div>
</section><section><div class="widget">
    <h1>Tags</h1><a href="../../../tags/bayesian.html">Bayesian</a> (4), <a href="../../../tags/blog_setup.html">blog setup</a> (1), <a href="../../../tags/bootstrap.html">bootstrap</a> (1), <a href="../../../tags/bottleneck.html">bottleneck</a> (1), <a href="../../../tags/caret.html">caret</a> (1), <a href="../../../tags/cmpy.html">cmpy</a> (1), <a href="../../../tags/conditional_probability.html">conditional probability</a> (3), <a href="../../../tags/coursera.html">coursera</a> (1), <a href="../../../tags/coursera_intro_to_data_science.html">coursera intro to data science</a> (3), <a href="../../../tags/cython.html">cython</a> (1), <a href="../../../tags/e1071.html">e1071</a> (1), <a href="../../../tags/garmin.html">garmin</a> (1), <a href="../../../tags/ggplot2.html">ggplot2</a> (1), <a href="../../../tags/git.html">git</a> (1), <a href="../../../tags/gnuplot.html">gnuplot</a> (1), <a href="../../../tags/graphs.html">graphs</a> (1), <a href="../../../tags/igraph.html">igraph</a> (1), <a href="../../../tags/ipython.html">ipython</a> (1), <a href="../../../tags/joint_probability.html">joint probability</a> (3), <a href="../../../tags/latex.html">LaTeX</a> (1), <a href="../../../tags/lda.html">LDA</a> (1), <a href="../../../tags/machine_learning.html">machine learning</a> (1), <a href="../../../tags/marginal_probability.html">marginal probability</a> (3), <a href="../../../tags/matplotlib.html">matplotlib</a> (1), <a href="../../../tags/my_python_setup.html">my python setup</a> (6), <a href="../../../tags/my_ubuntu_setup.html">my ubuntu setup</a> (10), <a href="../../../tags/networks.html">networks</a> (1), <a href="../../../tags/networkx.html">networkx</a> (1), <a href="../../../tags/numexpr.html">numexpr</a> (1), <a href="../../../tags/numpy.html">numpy</a> (1), <a href="../../../tags/octave.html">octave</a> (1), <a href="../../../tags/openpyxl.html">openpyxl</a> (1), <a href="../../../tags/pandas.html">pandas</a> (1), <a href="../../../tags/patsy.html">patsy</a> (1), <a href="../../../tags/pip.html">pip</a> (1), <a href="../../../tags/pweave.html">pweave</a> (1), <a href="../../../tags/pygraphviz.html">pygraphviz</a> (1), <a href="../../../tags/pymc.html">pymc</a> (1), <a href="../../../tags/python.html">Python</a> (1), <a href="../../../tags/python.html">python</a> (3), <a href="../../../tags/python_2_7.html">python 2.7</a> (5), <a href="../../../tags/python_3_4.html">python 3.4</a> (1), <a href="../../../tags/pyyaml.html">pyyaml</a> (1), <a href="../../../tags/r.html">R</a> (1), <a href="../../../tags/randomforest.html">randomForest</a> (1), <a href="../../../tags/restview.html">restview</a> (1), <a href="../../../tags/resume.html">resume</a> (1), <a href="../../../tags/rpart.html">rpart</a> (1), <a href="../../../tags/running.html">running</a> (1), <a href="../../../tags/scikit_learn.html">scikit-learn</a> (1), <a href="../../../tags/scipy.html">scipy</a> (1), <a href="../../../tags/screen.html">screen</a> (1), <a href="../../../tags/server_setup.html">server setup</a> (1), <a href="../../../tags/social_networks.html">social networks</a> (1), <a href="../../../tags/sphinx.html">sphinx</a> (1), <a href="../../../tags/sql.html">sql</a> (1), <a href="../../../tags/sqlite3.html">sqlite3</a> (1), <a href="../../../tags/ssh.html">ssh</a> (1), <a href="../../../tags/ssh_keys.html">ssh keys</a> (1), <a href="../../../tags/statsmodels.html">statsmodels</a> (1), <a href="../../../tags/sympy.html">sympy</a> (1), <a href="../../../tags/tableau.html">tableau</a> (1), <a href="../../../tags/tinkerer.html">tinkerer</a> (1), <a href="../../../tags/topic_models.html">topic models</a> (1), <a href="../../../tags/tree.html">tree</a> (1), <a href="../../../tags/ubuntu_14_04.html">ubuntu 14.04</a> (7), <a href="../../../tags/vim.html">vim</a> (1), <a href="../../../tags/virtualbox.html">virtualbox</a> (1), <a href="../../../tags/virtualenv.html">virtualenv</a> (3), <a href="../../../tags/virtualenvwrapper.html">virtualenvwrapper</a> (2), <a href="../../../tags/vps.html">VPS</a> (1), <a href="../../../tags/yaml.html">yaml</a> (1)</div></section></aside></div> <!-- #main --></div> <!-- #main-container -->

        <div class="footer-container" role="contentinfo"><footer class="wrapper">&copy; Copyright 2014 Christopher Strelioff. Powered by <a href="http://www.tinkerer.me/">Tinkerer</a> and <a href="http://sphinx.pocoo.org/">Sphinx</a>.</footer></div> <!-- footer-container -->

      </div> <!--! end of #container --><!--[if lt IE 7 ]>
          <script src="//ajax.googleapis.com/ajax/libs/chrome-frame/1.0.3/CFInstall.min.js"></script>
          <script>window.attachEvent('onload',function(){CFInstall.check({mode:'overlay'})})</script>
        <![endif]-->
    </body>
</html>