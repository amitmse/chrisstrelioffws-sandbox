<!DOCTYPE html><!--[if lt IE 7]>      <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js"> <!--<![endif]-->
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="description" content="Christopher Strelioff's blog">
        <meta name="viewport" content="width=device-width">
        <title>Inferring probabilities with a Beta prior, a third example of Bayesian calculations &mdash; chris&#39; sandbox</title>
            <link rel="stylesheet" href="../../../_static/normalize.css" type="text/css">
            <link rel="stylesheet" href="../../../_static/sphinx.css" type="text/css">
            <link rel="stylesheet" href="../../../_static/main.css" type="text/css">
            <link rel="stylesheet" href="../../../_static/flat.css" type="text/css">
            <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
            <link rel="stylesheet" href="../../../_static/font-awesome.min.css" type="text/css">
        <link rel="stylesheet" href="../../../_static/style.css" type="text/css" /><link rel="shortcut icon" href="../../../_static/tinkerer.ico" /><!-- Load modernizr and JQuery -->
        <script src="../../../_static/vendor/modernizr-2.6.2.min.js"></script>
        <script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.2/jquery.min.js"></script>
        <script>window.jQuery || document.write('<script src="../../../_static/vendor/jquery-1.8.2.min.js"><\/script>')</script>
        <script src="../../../_static/plugins.js"></script>
        <script src="../../../_static/main.js"></script>
        <link rel="next" title="Installing essentia for audio feature extraction" href="../10/installing_essentia_for_audio_feature_extraction.html" /><link rel="prev" title="Installing Node.js and npm on Ubuntu 14.04" href="../../../2015/01/12/installing_node_js_and_npm_on_ubuntu_14_04.html" /><link rel="alternate" type="application/rss+xml" title="RSS" href="../../../rss.html" /><script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     '1.5',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script><script type="text/javascript" src="../../../_static/underscore.js"></script><script type="text/javascript" src="../../../_static/doctools.js"></script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript" src="../../../_static/disqus.js"></script><script type="text/javascript" src="../../../_static/google_analytics.js"></script>

    <script type="text/javascript">
        $(document).ready(function () {
            // Scroll to content if on small screen
            if (screen.width < 480)
            {
                $(document).scrollTop(document.getElementsByTagName("article")[0].offsetTop - 44);
            }
        });
    </script></head>
    <body role="document">
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> or <a href="http://www.google.com/chromeframe/?redirect=true">activate Google Chrome Frame</a> to improve your experience.</p>
        <![endif]-->

      <div id="container"><header role="banner">
    <hgroup>
      <h1><a href="../../../index.html">chris&#39; sandbox</a></h1><h2>python, R, ubuntu, bayesian methods, machine learning and more...</h2></hgroup>
  </header>
<div class="main-container" role="main"><div class="main wrapper body clearfix"><article><div class="timestamp postmeta">
            <span>December 11, 2014</span>
        </div>
    <div class="section" id="inferring-probabilities-with-a-beta-prior-a-third-example-of-bayesian-calculations">
<span id="bayes-third-example"></span><h1>Inferring probabilities with a Beta prior, a third example of Bayesian calculations</h1>
<p>In this post I will expand on a previous example of inferring probabilities
from a data series: <a class="reference internal" href="../../10/24/inferring_probabilities_a_second_example_of_bayesian_calculations.html#bayes-second-example"><em>Inferring probabilities, a second example of Bayesian calculations</em></a>. In particular, instead of
considering a discrete set of candidate probabilities, I&#8217;ll consider all
(continuous) values between <span class="math">\(0\)</span> and <span class="math">\(1\)</span>.  This means our prior (and
posterior) will now be a <a class="reference external" href="http://en.wikipedia.org/wiki/Probability_density_function">probability density function</a> (pdf) instead of a
<a class="reference external" href="http://en.wikipedia.org/wiki/Probability_mass_function">probability mass function</a> (pmf).  More specifically, I&#8217;ll use the
<a class="reference external" href="http://en.wikipedia.org/wiki/Beta_distribution">Beta Distribution</a> for this example.</p>
<div id="more"> </div><p>In the post <a class="reference internal" href="../../10/24/inferring_probabilities_a_second_example_of_bayesian_calculations.html#bayes-second-example"><em>Inferring probabilities, a second example of Bayesian calculations</em></a> I considered inference of
<span class="math">\(p_{0}\)</span>, the probability for a zero, from a data series:</p>
<div class="math">
\[D = 0000110001\]</div>
<p><em>I&#8217;ll tackle the same question in this post</em> using a different prior for
<span class="math">\(p_{0}\)</span>, one that allows for continuous values between <span class="math">\(0\)</span> and
<span class="math">\(1\)</span> instead of a discrete set of candidates.  This makes the math a bit
more difficult, requiring integrals instead of sums, but the basic ideas are
the same.</p>
<p>This is one in a series of posts on Bayesian methods, starting from the basics
and increasing in difficulty:</p>
<ul class="simple">
<li><a class="reference internal" href="../../08/26/joint_conditional_and_marginal_probabilities.html#joint-conditional-and-marginal-probabilities"><em>Joint, conditional and marginal probabilities</em></a></li>
<li><a class="reference internal" href="../../09/11/medical_tests_a_first_example_of_bayesian_calculations.html#bayes-medical-tests"><em>Medical tests, a first example of Bayesian calculations</em></a></li>
<li><a class="reference internal" href="../../10/24/inferring_probabilities_a_second_example_of_bayesian_calculations.html#bayes-second-example"><em>Inferring probabilities, a second example of Bayesian calculations</em></a></li>
</ul>
<p>If the following is unfamiliar or difficult try consulting one or more of the
above posts for some more basic, introductory material.</p>
<div class="section" id="likelihood">
<h2>Likelihood</h2>
<p>The starting point for our inference problem is the <em>likelihood</em> &#8211; the
probability of the observed data series, written like I know the value of
<span class="math">\(p_{0}\)</span> (see the <a class="reference internal" href="../../10/24/inferring_probabilities_a_second_example_of_bayesian_calculations.html#bayes-second-example"><em>Inferring probabilities, a second example of Bayesian calculations</em></a> post for more details, I&#8217;ll
be brief here):</p>
<div class="math">
\[P(D=0000110001 \vert p_{0} ) = p_{0}^{7} \times (1-p_{0})^{3}\]</div>
<p>To be clear, we could plug in <span class="math">\(p_{0}=0.6\)</span>, and find the probability of
the specified data series given that value for the unknown probability:</p>
<div class="math">
\[P(D=0000110001 \vert p_{0}=0.6 ) = 0.6^{7} \times (1-0.6)^{3}\]</div>
<p>A more general form for the likelihood, not being specific about the data
series considered, is</p>
<div class="math">
\[P(D \vert p_{0} ) = p_{0}^{n{0}} \times (1-p_{0})^{n_{1}}\]</div>
<p>where <span class="math">\(n_{0}\)</span> is the number of zeros and <span class="math">\(n_{1}\)</span> is the number of
ones in whatever data series <span class="math">\(D\)</span> is considered.</p>
</div>
<div class="section" id="prior-the-beta-distribution">
<h2>Prior &#8211; The Beta Distribution</h2>
<p>The new material, relative to the previous post on this topic, starts here. We
use the <a class="reference external" href="http://en.wikipedia.org/wiki/Beta_distribution">Beta Distribution</a> to represent our prior assumptions/information.
The mathematical form is:</p>
<div class="math">
\[P(p_{0} \vert \alpha_{0}, \alpha_{1} )  =
  \frac{
    \Gamma(\alpha_{0} + \alpha_{1})
    }{
    \Gamma(\alpha_{0}) \Gamma(\alpha_{1})
    } \,
p_{0}^{\alpha_{0}-1} \, (1-p_{0})^{\alpha_{1}-1}\]</div>
<p>where <span class="math">\(\alpha_{0}\)</span> and <span class="math">\(\alpha_{1}\)</span> are hyper-parameters that we
have to set to reflect our prior assumptions/information about the value of
<span class="math">\(p_{0}\)</span>. I have found that
<em>a probability density funtion for a probability really confuses people</em>.
However, just think of <span class="math">\(p_{0}\)</span> as a parameter that we want to infer&#8211;
ignore the fact that the parameter is a probability.</p>
<p>In any case, it is worth spending some time getting used to this
prior and what it means. To help, I&#8217;ll go though some properties of the
<a class="reference external" href="http://en.wikipedia.org/wiki/Beta_distribution">Beta Distribution</a>.  However, note that the <strong>posterior pdf will also be a
Beta Distribution</strong>, so it is worth the effort to get comfortable with the pdf.</p>
<p>So, some properties are (some of this material will be technical in nature, so
skim it over at first and look at the Python code below if you find it too
mathy):</p>
<p><strong>Prior mean &#8211;</strong> Most people want a single number, or point estimate, to
represent the results of inference or the information contained in the
prior.  However, in a Bayesian approach to
inference the prior and posterior are both pdf&#8217;s or pmf&#8217;s.  One way to get a
point estimate is to take the <em>average</em> of the parameter of interest with
respect to the prior or posterior.  For example, for the Beta prior we
obtain:</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
  \mathbf{E}_{prior}[p_{0}] &amp; = &amp; \int_{0}^{1} \, dp_{0} \, p_{0} \,
                                  P(p_{0} \vert \alpha_{0}, \alpha_{1}) \\
  &amp; = &amp; \frac{\alpha_{0}}{\alpha_{0}+\alpha_{1}}
\end{array}\end{split}\]</div>
<p>However, note that this single number does not fully characterize the prior or
posterior and should be used with care.  Many other properties (higher moments,
variance, etc) can be calculated&#8211; see <a class="reference external" href="http://en.wikipedia.org/wiki/Beta_distribution">Beta Distribution</a> for more options.
Also checkout this nice post on <a class="reference external" href="http://sumsar.net/blog/2014/10/probable-points-and-credible-intervals-part-one/">Probable Points and Credible Intervals</a> for
ideas on how to summarize a posterior distribution (also relevant to priors).</p>
<p><strong>The pdf is normalized &#8211;</strong> This means if we integrate <span class="math">\(p_{0}\)</span> from
<span class="math">\(0\)</span> to <span class="math">\(1\)</span> we get one:</p>
<div class="math">
\[\int_{0}^{1} \, dp_{0} \, P(p_{0} \vert \alpha_{0}, \alpha{1}) = 1\]</div>
<p>This is true because of the following relationship:</p>
<div class="math">
\[\int_{0}^{1} \, dp_{0} \, p_{0}^{\alpha_{0}-1} \, (1-p_{0})^{\alpha_{1}-1}
=
\frac{\Gamma(\alpha_{0}) \Gamma(\alpha_{1})
     }{\Gamma(\alpha_{0} + \alpha_{1})}\]</div>
<p>The above integral produces the <a class="reference external" href="http://en.wikipedia.org/wiki/Beta_function">Beta function</a> (the relation is also called
the Euler integral).  For our purposes, the most import information is the
<a class="reference external" href="http://en.wikipedia.org/wiki/Beta_distribution">Beta Distribution</a> is normalized on the <span class="math">\(0\)</span> to <span class="math">\(1\)</span> interval, as
necessary for a probability like <span class="math">\(p_{0}\)</span>.</p>
<p><strong>Prior assumptions and information can be reflected by setting
hyper-parameters &#8211;</strong> The hyper-parameters <span class="math">\(\alpha_{0}\)</span> and
<span class="math">\(\alpha_{1}\)</span> affect the shape of the pdf, enabling a flexible encoding
of prior information.</p>
<p>For example, no preferred values of <span class="math">\(p_{0}\)</span> can be reflected
by using <span class="math">\(\alpha_{0}=1\)</span>, <span class="math">\(\alpha_{1}=1\)</span>. This pdf looks like</p>
<a class="reference internal image-reference" href="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure2_1.png"><img alt="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure2_1.png" src="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure2_1.png" style="width: 15cm;" /></a>
<p>Another prior could assign <span class="math">\(\alpha_{0}=5\)</span>, <span class="math">\(\alpha_{1}=5\)</span>, which
prefers values near <span class="math">\(p_{0}=1/2\)</span> and looks like</p>
<a class="reference internal image-reference" href="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure3_1.png"><img alt="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure3_1.png" src="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure3_1.png" style="width: 15cm;" /></a>
<p>Finally, we can get non-symmetric priors using <span class="math">\(\alpha_{0} \neq
\alpha_{1}\)</span>, as can be seen with <span class="math">\(\alpha_{0}=2\)</span> and
<span class="math">\(\alpha_{1}=8\)</span>:</p>
<a class="reference internal image-reference" href="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure4_1.png"><img alt="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure4_1.png" src="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure4_1.png" style="width: 15cm;" /></a>
<p>Some things to remember about setting the hyper-parameters:</p>
<ul class="simple">
<li>If <span class="math">\(\alpha_{0}=\alpha_{1}\)</span> the prior will be symmetric with prior mean
equal to <span class="math">\(\mathbf{E}_{prior}[p_{0}] = 1/2\)</span>.</li>
<li>If <span class="math">\(\alpha_{0} \neq \alpha_{1}\)</span> the prior will be asymmetric with a
prior mean different from <span class="math">\(1/2\)</span>.</li>
<li>The <em>strength</em> of the prior is related to the sum
<span class="math">\(\alpha_{0}+\alpha_{1}\)</span>. Compare the alpha sum with
<span class="math">\(n_{0} + n_{1}\)</span> from the data, treating the alpha&#8217;s as fake counts.
The relative size of these sums controls the effects of the prior and
likelihood on the shape of the posterior.  This will become clear in the
Python examples below.</li>
</ul>
<p><strong>The cumulative distribution function (cdf) &#8211;</strong> The cdf (see <a class="reference external" href="http://en.wikipedia.org/wiki/Cumulative_distribution_function">cumulative
distribution function</a> at wikipedia for more info) for the
<a class="reference external" href="http://en.wikipedia.org/wiki/Beta_distribution">Beta Distribution</a> let&#8217;s us calculate the probability that <span class="math">\(p_{0}\)</span> is
less than or equal to a value <span class="math">\(x\)</span>.  To be specific, the cdf is defined:</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
P(p_{0} \leq x \vert \alpha_{0}, \alpha_{1})
&amp; = &amp;
\int_{0}^{x} P(p_{0} \vert \alpha_{0}, \alpha_{1} ) \\
&amp; = &amp; I_{x}(\alpha_{0}, \alpha_{1})
\end{array}\end{split}\]</div>
<p>The integral is also called the <em>incomplete Beta ingtegral</em> and denoted
<span class="math">\(I_{x}(\alpha_{0}, \alpha_{1})\)</span>.</p>
<p>If we want the probability that <span class="math">\(p_{0}\)</span> is between the values
<span class="math">\(x_{l}\)</span> and <span class="math">\(x_{h}\)</span> we can use the cdf to calculate this:</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
P(x_{l} \lt p_{0} \leq x_{h} \vert \alpha_{0}, \alpha_{1})
&amp; = &amp;
P(p_{0} \leq x_{h} \vert \alpha_{0}, \alpha_{1}) \\
&amp; - &amp;
P(p_{0} \leq x_{l} \vert \alpha_{0}, \alpha_{1}) \\
&amp; = &amp; I_{x_{h}}(\alpha_{0}, \alpha_{1})
    - I_{x_{l}}(\alpha_{0}, \alpha_{1})
\end{array}\end{split}\]</div>
<p>The incomplete Beta integral, or cdf, and it&#8217;s inverse allows for the
calculation of a credible interval from the prior or posterior.  Using these
tools the value of <span class="math">\(p_{0}\)</span> can be said to be within a certain range with
95% probability&#8211; again, we&#8217;ll use Python code to plot this below.</p>
<p><strong>The Beta Distribution is a conjugate prior for this problem &#8211;</strong> This means
that the posterior will have the same mathematical form as the prior (it will
also be a <a class="reference external" href="http://en.wikipedia.org/wiki/Beta_distribution">Beta Distribution</a>) with updated hyper-parameters.  This
mathematical &#8216;resonance&#8217; is really nice and let&#8217;s us do full Bayesian inference
without MCMC.</p>
<p>Okay, enough about the prior and the <a class="reference external" href="http://en.wikipedia.org/wiki/Beta_distribution">Beta Distribution</a>, now let&#8217;s talk about
Bayes&#8217; Theorem and the posterior pdf for this problem.</p>
</div>
<div class="section" id="bayes-theorem-and-the-posterior">
<h2>Bayes&#8217; Theorem and the Posterior</h2>
<p>Our final goal is the posterior probability density function, combining the
likelihood and the prior to make an updated reflection of our
knowledge of <span class="math">\(p_{0}\)</span> after considering data. The posterior pdf has the
form (in this case):</p>
<div class="math">
\[P(p_{0} \vert D, \alpha_{0}, \alpha_{1})\]</div>
<p>In words, this is <em>the probability density for</em> <span class="math">\(p_{0}\)</span> <em>given data
series</em> <span class="math">\(D\)</span> <em>and prior assumptions, reflected by the Beta pdf with
hyper-parameters</em> <span class="math">\((\alpha_{0}, \alpha_{1})\)</span>.</p>
<p>In this setting Bayes&#8217; Theorem takes the form:</p>
<div class="math">
\[\color{blue}{P(p_{0} \vert D, \alpha_{0}, \alpha_{1})}
= \frac{P(D \vert p_{0})
  \color{red}{P(p_{0} \vert \alpha_{0}, \alpha_{1})}
  }{
  \int_{0}^{1} \, d\hat{p}_{0} \,
  P(D \vert \hat{p}_{0})
  \color{red}{P(\hat{p}_{0} \vert \alpha_{0}, \alpha_{1})}
  }\]</div>
<p>where the posterior
<span class="math">\(\color{blue}{P(p_{0} \vert D, \alpha_{0}, \alpha_{1})}\)</span> is blue, the
likelihood <span class="math">\(P(D \vert p_{0})\)</span> is black, and the prior
<span class="math">\(\color{red}{P(p_{0} \vert \alpha_{0}, \alpha_{1})}\)</span> is red.
Notice that the normalizing <em>marginal likelihood</em> or <em>evidence</em> (denominator in
the above equation) is now an integral.  This is the price of using continuous
values for <span class="math">\(p_{0}\)</span>&#8211; you should compare this with Bayes&#8217; Theorem in the
<a class="reference internal" href="../../10/24/inferring_probabilities_a_second_example_of_bayesian_calculations.html#bayes-second-example"><em>Inferring probabilities, a second example of Bayesian calculations</em></a> post.</p>
<p>As always, try to think about Bayes&#8217; Theorem as information about <span class="math">\(p_{0}\)</span>
being updated from <strong>assumptions</strong> (<span class="math">\(\alpha_{0}, \alpha_{1}\)</span>)
to <strong>assumptions + data</strong> (<span class="math">\(D, \alpha_{0}, \alpha_{1}\)</span>):</p>
<div class="math">
\[\color{red}{P(p_{0} \vert \alpha_{0}, \alpha_{1})}
\rightarrow
\color{blue}{P(p_{0} \vert D, \alpha_{0}, \alpha_{1})}\]</div>
<p>To get the posterior pdf, we have to do the integral in the denominator of
Bayes&#8217; Theorem.  In this case, the calculation is possible, using the
properties of the <a class="reference external" href="http://en.wikipedia.org/wiki/Beta_distribution">Beta Distribution</a>.  The integral goes as follows:</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
P(D \vert \alpha_{0}, \alpha_{1})
&amp; = &amp;
  \int_{0}^{1} \, d\hat{p}_{0} \,
  P(D \vert \hat{p}_{0})
  P(\hat{p}_{0} \vert \alpha_{0}, \alpha_{1}) \\
&amp; &amp; \\
&amp; = &amp;  \int_{0}^{1} \, d\hat{p}_{0} \,
       \hat{p}_{0}^{n_{0}} \, (1-\hat{p}_{0})^{n_{1}} \\
&amp; \times &amp;
  \frac{ \Gamma(\alpha_{0} + \alpha_{1})
    }{
    \Gamma(\alpha_{0}) \Gamma(\alpha_{1}) }
    \hat{p}_{0}^{\alpha_{0}-1} (1-\hat{p}_{0})^{\alpha_{1}-1} \\
&amp; &amp; \\
&amp; = &amp;
  \frac{ \Gamma(\alpha_{0} + \alpha_{1})
   }{
   \Gamma(\alpha_{0}) \Gamma(\alpha_{1}) } \\
&amp; \times &amp;
   \int_{0}^{1} \, d\hat{p}_{0} \,
   \hat{p}_{0}^{\alpha_{0}+n_{0}-1} \, (1-\hat{p}_{0})^{\alpha_{1}+n_{1}-1}
\end{array}\end{split}\]</div>
<p>The integral on the last line defines a <a class="reference external" href="http://en.wikipedia.org/wiki/Beta_function">Beta Function</a>, as discussed in
the section on the prior, and has a known result:</p>
<div class="math">
\[\int_{0}^{1} \, dp_{0} \, p_{0}^{\alpha_{0}+n_{0}-1}
\, (1-p_{0})^{\alpha_{1}+n_{1}-1}
=
\frac{\Gamma(\alpha_{0}+n_{0}) \Gamma(\alpha_{1}+n_{1})
     }{\Gamma(\alpha_{0} + \alpha_{1} + n_{0} + n_{1})}\]</div>
<p>This means the denominator, also called the <strong>marginal likelihood</strong> or
<strong>evidence</strong>, is equal to:</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
P(D \vert \alpha_{0}, \alpha_{1})
&amp; = &amp;
  \frac{ \Gamma(\alpha_{0} + \alpha_{1})
   }{
   \Gamma(\alpha_{0}) \Gamma(\alpha_{1}) } \\
&amp; \times &amp;
  \frac{\Gamma(\alpha_{0}+n_{0}) \Gamma(\alpha_{1}+n_{1})
   }{
   \Gamma(\alpha_{0} + \alpha_{1} + n_{0} + n_{1})}
\end{array}\end{split}\]</div>
<p>If we plug all of this back into Bayes&#8217; Theorem we get another <a class="reference external" href="http://en.wikipedia.org/wiki/Beta_distribution">Beta
Distribution</a> for the <strong>posterior pdf</strong>, as promised above:</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
P(p_{0} \vert D, \alpha_{0}, \alpha_{1} )
&amp; =  &amp;
  \frac{
    \Gamma(\alpha_{0} + \alpha_{1} + n_{0} + n_{1})
    }{
    \Gamma(\alpha_{0}+n_{0}) \Gamma(\alpha_{1}+n_{1})
    } \\
&amp; \times &amp;
  p_{0}^{\alpha_{0}+n_{0}-1} \, (1-p_{0})^{\alpha_{1}+n_{1}-1}
\end{array}\end{split}\]</div>
<p>Again, we obtain this result because the <a class="reference external" href="http://en.wikipedia.org/wiki/Beta_distribution">Beta Distribution</a> is a conjugate
prior for the <a class="reference external" href="http://en.wikipedia.org/wiki/Bernoulli_process">Bernoulli Process</a> likelihood that we are considering.  Notice
that the hyper-parameters from the prior have been updated by count data</p>
<div class="math">
\[(\alpha_{0}, \alpha_{1})
\rightarrow
(\alpha_{0}+n_{0}, \alpha_{1}+n_{1})\]</div>
<p>This is exactly as one might expect without doing all of the math. In any case,
before moving to implementing this in Python, a couple of notes:</p>
<ul class="simple">
<li>The posterior pdf is normalized on the <span class="math">\(0\)</span> to <span class="math">\(1\)</span> interval, just
as we need for inferring a probability like <span class="math">\(p_{0}\)</span>.</li>
<li>The posterior mean, a way to give a point estimate of our inference is</li>
</ul>
<div class="math">
\[\begin{split}\begin{array}{ll}
  \mathbf{E}_{post}[p_{0}] &amp; = &amp; \int_{0}^{1} \, dp_{0} \, p_{0} \,
                           P(p_{0} \vert D, \alpha_{0}, \alpha_{1}) \\
  &amp; = &amp; \frac{\alpha_{0}+n_{0}}{\alpha_{0}+\alpha_{1}+n_{0}+n_{1}}
\end{array}\end{split}\]</div>
<ul class="simple">
<li>The cdf for the posterior is just like for the prior because we still have a
<a class="reference external" href="http://en.wikipedia.org/wiki/Beta_distribution">Beta Distribution</a> &#8211; except, now the parameters are updated with data.  In
any case, we can find credible intervals with the incomplete Beta integral
and it&#8217;s inverse, as discussed above.</li>
</ul>
</div>
<div class="section" id="inference-code-in-python">
<h2>Inference code in Python</h2>
<p><strong>Note:</strong> code available as <span class="code docutils literal"><span class="pre">ex003_bayes.py</span></span> at
<a class="reference external" href="https://github.com/cstrelioff/chrisstrelioffws-sandbox-examples">github examples repository</a>.</p>
<p>Let&#8217;s do some Python.  First, we do some import of packages that we will use
to calculate and plot prior, likelihood and posterior.  Notice that
<span class="code docutils literal"><span class="pre">scipy.stats</span></span> has a <span class="code docutils literal"><span class="pre">beta</span></span> class that we will use for the prior and
posterior pdfs.  Also, we use <span class="code docutils literal"><span class="pre">matplotlib</span></span> and the new styles, ggplot in
this case, to create some nice plots with minimal tweaking.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span><span class="p">,</span> <span class="n">print_function</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">beta</span>

<span class="c"># use matplotlib style sheet</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s">&#39;ggplot&#39;</span><span class="p">)</span>
<span class="k">except</span><span class="p">:</span>
    <span class="c"># version of matplotlib might not be recent</span>
    <span class="k">pass</span>
</pre></div>
</div>
<p><strong>Likelihood</strong></p>
<p>The likelihood is exactly the same as for the previous example&#8211; see
<a class="reference internal" href="../../10/24/inferring_probabilities_a_second_example_of_bayesian_calculations.html#bayes-second-example"><em>Inferring probabilities, a second example of Bayesian calculations</em></a>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">class</span> <span class="nc">likelihood</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Likelihood for binary data.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">counts</span> <span class="o">=</span> <span class="p">{</span><span class="n">s</span><span class="p">:</span><span class="mi">0</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="p">[</span><span class="s">&#39;0&#39;</span><span class="p">,</span> <span class="s">&#39;1&#39;</span><span class="p">]}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_process_data</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_process_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Process data.&quot;&quot;&quot;</span>
        <span class="n">temp</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="p">[</span><span class="s">&#39;0&#39;</span><span class="p">,</span> <span class="s">&#39;1&#39;</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">counts</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">temp</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">counts</span><span class="o">.</span><span class="n">values</span><span class="p">()):</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s">&quot;Passed data is not all 0`s and 1`s!&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_process_probabilities</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Process probabilities.&quot;&quot;&quot;</span>
        <span class="n">n0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">counts</span><span class="p">[</span><span class="s">&#39;0&#39;</span><span class="p">]</span>
        <span class="n">n1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">counts</span><span class="p">[</span><span class="s">&#39;1&#39;</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">p0</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">p0</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c"># typical case</span>
            <span class="n">logpr_data</span> <span class="o">=</span> <span class="n">n0</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p0</span><span class="p">)</span> <span class="o">+</span> \
                         <span class="n">n1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1.</span><span class="o">-</span><span class="n">p0</span><span class="p">)</span>
            <span class="n">pr_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logpr_data</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">p0</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">n0</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c"># p0 can&#39;t be 0 if n0 is not 0</span>
            <span class="n">logpr_data</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
            <span class="n">pr_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logpr_data</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">p0</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">n0</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c"># data consistent with p0=0</span>
            <span class="n">logpr_data</span> <span class="o">=</span> <span class="n">n1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1.</span><span class="o">-</span><span class="n">p0</span><span class="p">)</span>
            <span class="n">pr_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logpr_data</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">p0</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">n1</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c"># p0 can&#39;t be 1 if n1 is not 0</span>
            <span class="n">logpr_data</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
            <span class="n">pr_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logpr_data</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">p0</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">n1</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c"># data consistent with p0=1</span>
            <span class="n">logpr_data</span> <span class="o">=</span> <span class="n">n0</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p0</span><span class="p">)</span>
            <span class="n">pr_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logpr_data</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">pr_data</span><span class="p">,</span> <span class="n">logpr_data</span>

    <span class="k">def</span> <span class="nf">prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Get probability of data.&quot;&quot;&quot;</span>
        <span class="n">pr_data</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_probabilities</span><span class="p">(</span><span class="n">p0</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">pr_data</span>

    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Get log of probability of data.&quot;&quot;&quot;</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">logpr_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_probabilities</span><span class="p">(</span><span class="n">p0</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">logpr_data</span>
</pre></div>
</div>
<p><strong>Prior</strong></p>
<p>Our prior class will basically be a wrapper around <span class="code docutils literal"><span class="pre">scipy.stats.beta</span></span>
with a plotting method.  Notice that the <span class="code docutils literal"><span class="pre">plot()</span></span> method gets the Beta
Distribution mean and uses the <span class="code docutils literal"><span class="pre">interval()</span></span> method from
<span class="code docutils literal"><span class="pre">scipy.stats.beta</span></span> to get a region with 95% probability&#8211; this is done,
behind the scenes, using the incomplete Beta integral and it&#8217;s inverse as
discussed above.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">class</span> <span class="nc">prior</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha0</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha1</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Beta prior for binary data.&quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">a0</span> <span class="o">=</span> <span class="n">alpha0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a1</span> <span class="o">=</span> <span class="n">alpha1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p0rv</span> <span class="o">=</span> <span class="n">beta</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">a1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">interval</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prob</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;End points for region of pdf containing `prob` of the</span>
<span class="sd">        pdf-- this uses the cdf and inverse.</span>

<span class="sd">        Ex: interval(0.95)</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">p0rv</span><span class="o">.</span><span class="n">interval</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">mean</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns prior mean.&quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">p0rv</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">pdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Probability density at p0.&quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">p0rv</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">p0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;A plot showing mean and 95% credible interval.&quot;&quot;&quot;</span>

        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>

        <span class="c"># get prior mean p0</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="c"># get low/high pts containg 95% probability</span>
        <span class="n">low_p0</span><span class="p">,</span> <span class="n">high_p0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">interval</span><span class="p">(</span><span class="mf">0.95</span><span class="p">)</span>
        <span class="n">x_prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">low_p0</span><span class="p">,</span> <span class="n">high_p0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>

        <span class="c"># plot pdf</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s">&#39;r-&#39;</span><span class="p">)</span>

        <span class="c"># fill 95% region</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_prob</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_prob</span><span class="p">),</span>
                        <span class="n">color</span><span class="o">=</span><span class="s">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="s">&#39;0.2&#39;</span> <span class="p">)</span>

        <span class="c"># mean</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">stem</span><span class="p">([</span><span class="n">mean</span><span class="p">],</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">mean</span><span class="p">)],</span> <span class="n">linefmt</span><span class="o">=</span><span class="s">&#39;r-&#39;</span><span class="p">,</span>
                <span class="n">markerfmt</span><span class="o">=</span><span class="s">&#39;ro&#39;</span><span class="p">,</span> <span class="n">basefmt</span><span class="o">=</span><span class="s">&#39;w-&#39;</span><span class="p">)</span>

        <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">&#39;Probability of Zero&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">&#39;Prior PDF&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Let&#8217;s plot some Beta pdfs with a range of parameters using the new code. First,
the uniform prior</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">pri</span> <span class="o">=</span> <span class="n">prior</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">pri</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure8_1.png"><img alt="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure8_1.png" src="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure8_1.png" style="width: 15cm;" /></a>
<p>The vertical line with the dot shows the location of the mean of the pdf.  The
shaded region indicates the (symmetric) region with 95% probability for the
given values of <span class="math">\(\alpha_{0}\)</span> and <span class="math">\(\alpha_{1}\)</span>.  If you want the
actual values for the mean and the credible interval, these can be obtained as
well:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">print</span><span class="p">(</span><span class="s">&quot;Prior mean: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pri</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>
<span class="n">cred_int</span> <span class="o">=</span> <span class="n">pri</span><span class="o">.</span><span class="n">interval</span><span class="p">(</span><span class="mf">0.95</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">&quot;95% CI: {} -- {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cred_int</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cred_int</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre>Prior mean: 0.5
95% CI: 0.025 -- 0.975
</pre></div>
</div>
<p>The other prior examples from above also work:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">pri</span> <span class="o">=</span> <span class="n">prior</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">pri</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure10_1.png"><img alt="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure10_1.png" src="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure10_1.png" style="width: 15cm;" /></a>
<p>and</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">pri</span> <span class="o">=</span> <span class="n">prior</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="n">pri</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure11_1.png"><img alt="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure11_1.png" src="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure11_1.png" style="width: 15cm;" /></a>
<p>It&#8217;s useful to get a feel for the mean and uncertainty of prior assumptions, as
reflected by the hyper-parameters&#8211; try out some other values to build an
intuition.</p>
<p><strong>Posterior</strong></p>
<p>Finally, we build the class for the posterior.  As you might expect, I&#8217;ll take
data and a prior as arguments and extract the parameters needed for the
posterior from these elements.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">class</span> <span class="nc">posterior</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">prior</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;The posterior.</span>

<span class="sd">        data: a data sample as list</span>
<span class="sd">        prior: an instance of the beta prior class</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span> <span class="o">=</span> <span class="n">likelihood</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prior</span> <span class="o">=</span> <span class="n">prior</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_process_posterior</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_process_posterior</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Process the posterior using passed data and prior.&quot;&quot;&quot;</span>

        <span class="c"># extract n0, n1, a0, a1 from likelihood and prior</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span><span class="o">.</span><span class="n">counts</span><span class="p">[</span><span class="s">&#39;0&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span><span class="o">.</span><span class="n">counts</span><span class="p">[</span><span class="s">&#39;1&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">a0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">a1</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">p0rv</span> <span class="o">=</span> <span class="n">beta</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a0</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">n0</span><span class="p">,</span>
                         <span class="bp">self</span><span class="o">.</span><span class="n">a1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">n1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">interval</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prob</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;End points for region of pdf containing `prob` of the</span>
<span class="sd">        pdf.</span>

<span class="sd">        Ex: interval(0.95)</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">p0rv</span><span class="o">.</span><span class="n">interval</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">mean</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns posterior mean.&quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">p0rv</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">pdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Probability density at p0.&quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">p0rv</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">p0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;A plot showing prior, likelihood and posterior.&quot;&quot;&quot;</span>

        <span class="n">f</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>

        <span class="c">## Prior</span>
        <span class="c"># get prior mean p0</span>
        <span class="n">pri_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="c"># get low/high pts containg 95% probability</span>
        <span class="n">pri_low_p0</span><span class="p">,</span> <span class="n">pri_high_p0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">interval</span><span class="p">(</span><span class="mf">0.95</span><span class="p">)</span>
        <span class="n">pri_x_prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">pri_low_p0</span><span class="p">,</span> <span class="n">pri_high_p0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>

        <span class="c"># plot pdf</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s">&#39;r-&#39;</span><span class="p">)</span>

        <span class="c"># fill 95% region</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">pri_x_prob</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">pri_x_prob</span><span class="p">),</span>
                           <span class="n">color</span><span class="o">=</span><span class="s">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="s">&#39;0.2&#39;</span> <span class="p">)</span>

        <span class="c"># mean</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">stem</span><span class="p">([</span><span class="n">pri_mean</span><span class="p">],</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">pri_mean</span><span class="p">)],</span>
                   <span class="n">linefmt</span><span class="o">=</span><span class="s">&#39;r-&#39;</span><span class="p">,</span> <span class="n">markerfmt</span><span class="o">=</span><span class="s">&#39;ro&#39;</span><span class="p">,</span>
                   <span class="n">basefmt</span><span class="o">=</span><span class="s">&#39;w-&#39;</span><span class="p">)</span>

        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">&#39;Prior PDF&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>

        <span class="c">## Likelihood</span>
        <span class="c"># plot likelihood</span>
        <span class="n">lik</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="n">xi</span><span class="p">)</span> <span class="k">for</span> <span class="n">xi</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lik</span><span class="p">,</span> <span class="s">&#39;k-&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">&#39;Likelihood&#39;</span><span class="p">)</span>

        <span class="c">## Posterior</span>
        <span class="c"># get posterior mean p0</span>
        <span class="n">post_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="c"># get low/high pts containg 95% probability</span>
        <span class="n">post_low_p0</span><span class="p">,</span> <span class="n">post_high_p0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">interval</span><span class="p">(</span><span class="mf">0.95</span><span class="p">)</span>
        <span class="n">post_x_prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">post_low_p0</span><span class="p">,</span> <span class="n">post_high_p0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>

        <span class="c"># plot pdf</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s">&#39;b-&#39;</span><span class="p">)</span>

        <span class="c"># fill 95% region</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">post_x_prob</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">post_x_prob</span><span class="p">),</span>
                           <span class="n">color</span><span class="o">=</span><span class="s">&#39;blue&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="s">&#39;0.2&#39;</span> <span class="p">)</span>

        <span class="c"># mean</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">stem</span><span class="p">([</span><span class="n">post_mean</span><span class="p">],</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">post_mean</span><span class="p">)],</span>
                   <span class="n">linefmt</span><span class="o">=</span><span class="s">&#39;b-&#39;</span><span class="p">,</span> <span class="n">markerfmt</span><span class="o">=</span><span class="s">&#39;bo&#39;</span><span class="p">,</span>
                   <span class="n">basefmt</span><span class="o">=</span><span class="s">&#39;w-&#39;</span><span class="p">)</span>

        <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">&#39;Probability of Zero&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">&#39;Posterior PDF&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>That&#8217;s it with the base code, let do some examples.</p>
</div>
<div class="section" id="examples">
<h2>Examples</h2>
<p>Let&#8217;s start with an example using the data at the start of the post and a
uniform prior.  You can also compare the result with example from the previous
post using a set of candidate probabilities&#8211; <a class="reference internal" href="../../10/24/inferring_probabilities_a_second_example_of_bayesian_calculations.html#bayes-second-example"><em>Inferring probabilities, a second example of Bayesian calculations</em></a></p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># data</span>
<span class="n">data1</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>

<span class="c"># prior</span>
<span class="n">prior1</span> <span class="o">=</span> <span class="n">prior</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c"># posterior</span>
<span class="n">post1</span> <span class="o">=</span> <span class="n">posterior</span><span class="p">(</span><span class="n">data1</span><span class="p">,</span> <span class="n">prior1</span><span class="p">)</span>
<span class="n">post1</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure13_1.png"><img alt="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure13_1.png" src="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure13_1.png" style="width: 15cm;" /></a>
<p>Things to note here:</p>
<ul class="simple">
<li>The prior is uniform.  This means that the likelihood and the posterior have
the same shape.</li>
<li>The 95% credible interval is shown for both the prior and the posterior&#8211;
note how the information about <span class="math">\(p_{0}\)</span> has been updated with this short
data series.</li>
</ul>
<p>Next, let&#8217;s consider the same data with a prior that is not uniform. The
dataset is length 10, so <span class="math">\(n_{0}+n_{1}=10\)</span>.  Let&#8217;s set a prior with
<span class="math">\(\alpha_{0}+\alpha_{1}=10\)</span> but the prior is peaked in a different
location from the likelihood (maybe an expert has said this should be the prior
setting):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># prior</span>
<span class="n">prior2</span> <span class="o">=</span> <span class="n">prior</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>

<span class="c"># posterior</span>
<span class="n">post2</span> <span class="o">=</span> <span class="n">posterior</span><span class="p">(</span><span class="n">data1</span><span class="p">,</span> <span class="n">prior2</span><span class="p">)</span>
<span class="n">post2</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure14_1.png"><img alt="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure14_1.png" src="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure14_1.png" style="width: 15cm;" /></a>
<p>Well, obviously the data and the expert disagree at this point.  However,
because the prior is set with weight 10 and the data series is length 10 the
posterior peaks midway between the peaks in the prior and likelihood.
Try playing with this effect to better understand the interplay between the
prior hyper-parameters, the length of the dataset and the resulting posterior.</p>
<p>As a final example we consider two variants of the last example from
<a class="reference internal" href="../../10/24/inferring_probabilities_a_second_example_of_bayesian_calculations.html#bayes-second-example"><em>Inferring probabilities, a second example of Bayesian calculations</em></a>.  First we use a uniform prior:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># set probability of 0</span>
<span class="n">p0</span> <span class="o">=</span> <span class="mf">0.23</span>
<span class="c"># set rng seed to 42</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="c"># generate data</span>
<span class="n">data2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="mi">500</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="p">[</span><span class="n">p0</span><span class="p">,</span> <span class="mf">1.</span><span class="o">-</span><span class="n">p0</span><span class="p">])</span>

<span class="c"># prior</span>
<span class="n">prior3</span> <span class="o">=</span> <span class="n">prior</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="c"># posterior</span>
<span class="n">post3</span> <span class="o">=</span> <span class="n">posterior</span><span class="p">(</span><span class="n">data2</span><span class="p">,</span> <span class="n">prior3</span><span class="p">)</span>
<span class="n">post3</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure15_1.png"><img alt="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure15_1.png" src="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure15_1.png" style="width: 15cm;" /></a>
<p>Notice that the likelihood and posterior peak in the same place, just as we
would expect.  However, the peak is much stronger due to the longer dataset
(500 values).</p>
<p>Finally we use a &#8216;bad prior&#8217; on the same dataset.  In this case we&#8217;ll keep the
strength of the prior at 10, that is <span class="math">\(\alpha_{0}+\alpha_{1}=10\)</span>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># prior</span>
<span class="n">prior4</span> <span class="o">=</span> <span class="n">prior</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>

<span class="c"># posterior</span>
<span class="n">post4</span> <span class="o">=</span> <span class="n">posterior</span><span class="p">(</span><span class="n">data2</span><span class="p">,</span> <span class="n">prior4</span><span class="p">)</span>
<span class="n">post4</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure16_1.png"><img alt="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure16_1.png" src="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure16_1.png" style="width: 15cm;" /></a>
<p>Notice that the likelihood and posterior are very similar despite the prior
that peaks in the wrong place.  This examples demonstrates that a reasonable
amount of data should produce decent inference if the prior has not been set
too strong.  In general it&#8217;s good to have
<span class="math">\(n_{0}+n_{1} \gt \alpha_{0} + \alpha_{1}\)</span> and to consider the shapes of
both the prior and posterior.</p>
<p>That&#8217;s it for this post.  As always, comments, questions, and corrections as
welcome!</p>
</div>
</div>

    <div class="postmeta">
        <div class="author">
            <span>Posted by Chris Strelioff</span>
        </div>
        
        <div class="tags">
            <span>
                Tags:
                <a href="../../../tags/joint_probability.html">joint probability</a>, <a href="../../../tags/conditional_probability.html">conditional probability</a>, <a href="../../../tags/marginal_probability.html">marginal probability</a>, <a href="../../../tags/bayesian.html">Bayesian</a>, <a href="../../../tags/python.html">python</a>, <a href="../../../tags/beta.html">Beta</a></span>
        </div>
        </div><ul class="related clearfix">
            <li class="left"> &laquo; <a href="../../../2015/01/12/installing_node_js_and_npm_on_ubuntu_14_04.html">Installing Node.js and npm on Ubuntu 14.04</a></li>
            <li class="right"><a href="../10/installing_essentia_for_audio_feature_extraction.html">Installing essentia for audio feature extraction</a> &raquo; </li>
        </ul><div id="disqus_thread"></div><script type="text/javascript">    var disqus_shortname = "chrissandbox";    var disqus_identifier = "2014/12/11/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations";    disqus_thread();</script><noscript>Please enable JavaScript to view the    <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></article><aside class="sidebar"><section><div class="widget" id="searchbox" role="search">
    <h1><a href="#searchbox">Search</a></h1>
    <form action="../../../search.html" method="get">
        <input type="text" name="q" />
        <button type="submit"><span class="fa fa-search"></span></button>
    </form>
</div></section><section><div class="widget">
  <h1>Pages</h1>
  <ul>
  <li>
      <a href="../../../index.html">Home</a>
    </li>
  </ul>
</div>
</section><section><div class="widget">
    <h1>Recent Posts</h1>
    <ul><li>
            <a href="../../../2015/03/03/installing_pysoundfile_on_ubuntu_14_04.html">Installing PySoundFile on Ubuntu 14.04</a>
        </li><li>
            <a href="../../../2015/02/17/install_qgis_on_ubuntu_14_04.html">Install QGIS on Ubuntu 14.04</a>
        </li><li>
            <a href="../../../2015/02/17/using_python_to_query_data_from_socrata.html">Using Python to query data from Socrata</a>
        </li><li>
            <a href="../../../2015/01/12/installing_node_js_and_npm_on_ubuntu_14_04.html">Installing Node.js and npm on Ubuntu 14.04</a>
        </li><li>
            <a href="#">Inferring probabilities with a Beta prior, a third example of Bayesian calculations</a>
        </li><li>
            <a href="../10/installing_essentia_for_audio_feature_extraction.html">Installing essentia for audio feature extraction</a>
        </li><li>
            <a href="../../11/13/getting_started_with_latent_dirichlet_allocation_in_python.html">Getting started with Latent Dirichlet Allocation in Python</a>
        </li><li>
            <a href="../../10/24/inferring_probabilities_a_second_example_of_bayesian_calculations.html">Inferring probabilities, a second example of Bayesian calculations</a>
        </li><li>
            <a href="../../09/16/python_3_4_on_ubuntu_14_04_using_virtual_environments.html">Python 3.4 on Ubuntu 14.04 using virtual environments</a>
        </li><li>
            <a href="../../09/11/medical_tests_a_first_example_of_bayesian_calculations.html">Medical tests, a first example of Bayesian calculations</a>
        </li></ul>
</div>
</section><section><div class="widget">
    <h1>Tags</h1><a href="../../../tags/api.html">api</a> (1), <a href="../../../tags/audio.html">audio</a> (1), <a href="../../../tags/audio_features.html">audio features</a> (1), <a href="../../../tags/bayesian.html">Bayesian</a> (5), <a href="../../../tags/beta.html">Beta</a> (1), <a href="../../../tags/blog_setup.html">blog setup</a> (1), <a href="../../../tags/bootstrap.html">bootstrap</a> (1), <a href="../../../tags/bottleneck.html">bottleneck</a> (1), <a href="../../../tags/c.html">c++</a> (1), <a href="../../../tags/caret.html">caret</a> (1), <a href="../../../tags/cmpy.html">cmpy</a> (1), <a href="../../../tags/conditional_probability.html">conditional probability</a> (4), <a href="../../../tags/coursera.html">coursera</a> (1), <a href="../../../tags/coursera_intro_to_data_science.html">coursera intro to data science</a> (3), <a href="../../../tags/cython.html">cython</a> (1), <a href="../../../tags/d3.html">d3</a> (1), <a href="../../../tags/dsp.html">dsp</a> (1), <a href="../../../tags/e1071.html">e1071</a> (1), <a href="../../../tags/essentia.html">essentia</a> (1), <a href="../../../tags/garmin.html">garmin</a> (1), <a href="../../../tags/ggplot2.html">ggplot2</a> (1), <a href="../../../tags/gis.html">gis</a> (1), <a href="../../../tags/git.html">git</a> (1), <a href="../../../tags/gnuplot.html">gnuplot</a> (1), <a href="../../../tags/graphs.html">graphs</a> (1), <a href="../../../tags/igraph.html">igraph</a> (1), <a href="../../../tags/ipython.html">ipython</a> (1), <a href="../../../tags/javascript.html">javascript</a> (1), <a href="../../../tags/joint_probability.html">joint probability</a> (4), <a href="../../../tags/json.html">json</a> (1), <a href="../../../tags/latex.html">LaTeX</a> (1), <a href="../../../tags/lda.html">LDA</a> (1), <a href="../../../tags/machine_learning.html">machine learning</a> (1), <a href="../../../tags/marginal_probability.html">marginal probability</a> (4), <a href="../../../tags/matplotlib.html">matplotlib</a> (1), <a href="../../../tags/mir.html">mir</a> (1), <a href="../../../tags/music.html">music</a> (1), <a href="../../../tags/my_python_setup.html">my python setup</a> (6), <a href="../../../tags/my_ubuntu_setup.html">my ubuntu setup</a> (10), <a href="../../../tags/networks.html">networks</a> (1), <a href="../../../tags/networkx.html">networkx</a> (1), <a href="../../../tags/nodejs.html">nodejs</a> (1), <a href="../../../tags/npm.html">npm</a> (1), <a href="../../../tags/numexpr.html">numexpr</a> (1), <a href="../../../tags/numpy.html">numpy</a> (1), <a href="../../../tags/octave.html">octave</a> (1), <a href="../../../tags/open_oakland.html">Open Oakland</a> (2), <a href="../../../tags/openpyxl.html">openpyxl</a> (1), <a href="../../../tags/pandas.html">pandas</a> (1), <a href="../../../tags/patsy.html">patsy</a> (1), <a href="../../../tags/pip.html">pip</a> (1), <a href="../../../tags/pweave.html">pweave</a> (1), <a href="../../../tags/pygraphviz.html">pygraphviz</a> (1), <a href="../../../tags/pymc.html">pymc</a> (1), <a href="../../../tags/pysoundfile.html">PySoundFile</a> (1), <a href="../../../tags/python.html">python</a> (7), <a href="../../../tags/python.html">Python</a> (1), <a href="../../../tags/python_2_7.html">python 2.7</a> (5), <a href="../../../tags/python_3_4.html">python 3.4</a> (1), <a href="../../../tags/pyyaml.html">pyyaml</a> (1), <a href="../../../tags/qgis.html">qgis</a> (1), <a href="../../../tags/r.html">R</a> (1), <a href="../../../tags/randomforest.html">randomForest</a> (1), <a href="../../../tags/restview.html">restview</a> (1), <a href="../../../tags/resume.html">resume</a> (1), <a href="../../../tags/rpart.html">rpart</a> (1), <a href="../../../tags/running.html">running</a> (1), <a href="../../../tags/scikit_learn.html">scikit-learn</a> (1), <a href="../../../tags/scipy.html">scipy</a> (1), <a href="../../../tags/screen.html">screen</a> (1), <a href="../../../tags/server_setup.html">server setup</a> (1), <a href="../../../tags/social_networks.html">social networks</a> (1), <a href="../../../tags/socrata.html">Socrata</a> (1), <a href="../../../tags/sound.html">sound</a> (1), <a href="../../../tags/sphinx.html">sphinx</a> (1), <a href="../../../tags/sql.html">sql</a> (1), <a href="../../../tags/sqlite3.html">sqlite3</a> (1), <a href="../../../tags/ssh.html">ssh</a> (1), <a href="../../../tags/ssh_keys.html">ssh keys</a> (1), <a href="../../../tags/statsmodels.html">statsmodels</a> (1), <a href="../../../tags/sympy.html">sympy</a> (1), <a href="../../../tags/tableau.html">tableau</a> (1), <a href="../../../tags/tinkerer.html">tinkerer</a> (1), <a href="../../../tags/topic_models.html">topic models</a> (1), <a href="../../../tags/tree.html">tree</a> (1), <a href="../../../tags/ubuntu_14_04.html">ubuntu 14.04</a> (8), <a href="../../../tags/vim.html">vim</a> (1), <a href="../../../tags/virtualbox.html">virtualbox</a> (1), <a href="../../../tags/virtualenv.html">virtualenv</a> (3), <a href="../../../tags/virtualenvwrapper.html">virtualenvwrapper</a> (2), <a href="../../../tags/vps.html">VPS</a> (1), <a href="../../../tags/yaml.html">yaml</a> (1)</div></section></aside></div> <!-- #main --></div> <!-- #main-container -->

        <div class="footer-container" role="contentinfo"><footer class="wrapper">&copy; Copyright 2014 Christopher Strelioff. Powered by <a href="http://www.tinkerer.me/">Tinkerer</a> and <a href="http://sphinx.pocoo.org/">Sphinx</a>.</footer></div> <!-- footer-container -->

      </div> <!--! end of #container --><!--[if lt IE 7 ]>
          <script src="//ajax.googleapis.com/ajax/libs/chrome-frame/1.0.3/CFInstall.min.js"></script>
          <script>window.attachEvent('onload',function(){CFInstall.check({mode:'overlay'})})</script>
        <![endif]-->
    </body>
</html>