<!DOCTYPE html><!--[if lt IE 7]>      <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js"> <!--<![endif]-->
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="description" content="Christopher Strelioff's blog">
        <meta name="viewport" content="width=device-width">
        <title>Inferring probabilities with a Beta prior, a third example of Bayesian calculations &mdash; chris&#39; sandbox</title>
            <link rel="stylesheet" href="../../../_static/normalize.css" type="text/css">
            <link rel="stylesheet" href="../../../_static/sphinx.css" type="text/css">
            <link rel="stylesheet" href="../../../_static/main.css" type="text/css">
            <link rel="stylesheet" href="../../../_static/flat.css" type="text/css">
            <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
            <link rel="stylesheet" href="../../../_static/font-awesome.min.css" type="text/css">
        <link rel="stylesheet" href="../../../_static/style.css" type="text/css" /><link rel="shortcut icon" href="../../../_static/tinkerer.ico" /><!-- Load modernizr and JQuery -->
        <script src="../../../_static/vendor/modernizr-2.6.2.min.js"></script>
        <script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.2/jquery.min.js"></script>
        <script>window.jQuery || document.write('<script src="../../../_static/vendor/jquery-1.8.2.min.js"><\/script>')</script>
        <script src="../../../_static/plugins.js"></script>
        <script src="../../../_static/main.js"></script>
        <link rel="next" title="Installing essentia for audio feature extraction" href="../10/installing_essentia_for_audio_feature_extraction.html" /><link rel="alternate" type="application/rss+xml" title="RSS" href="../../../rss.html" /><script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     '1.5',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script><script type="text/javascript" src="../../../_static/underscore.js"></script><script type="text/javascript" src="../../../_static/doctools.js"></script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript" src="../../../_static/disqus.js"></script><script type="text/javascript" src="../../../_static/google_analytics.js"></script>

    <script type="text/javascript">
        $(document).ready(function () {
            // Scroll to content if on small screen
            if (screen.width < 480)
            {
                $(document).scrollTop(document.getElementsByTagName("article")[0].offsetTop - 44);
            }
        });
    </script></head>
    <body role="document">
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> or <a href="http://www.google.com/chromeframe/?redirect=true">activate Google Chrome Frame</a> to improve your experience.</p>
        <![endif]-->

      <div id="container"><header role="banner">
    <hgroup>
      <h1><a href="../../../index.html">chris&#39; sandbox</a></h1><h2>python, R, ubuntu, bayesian methods, machine learning and more...</h2></hgroup>
  </header>
<div class="main-container" role="main"><div class="main wrapper body clearfix"><article><div class="timestamp postmeta">
            <span>December 11, 2014</span>
        </div>
    <div class="section" id="inferring-probabilities-with-a-beta-prior-a-third-example-of-bayesian-calculations">
<span id="bayes-third-example"></span><h1>Inferring probabilities with a Beta prior, a third example of Bayesian calculations</h1>
<p>In this post I will expand on a previous example of inferring probabilities
from a data series: <a class="reference internal" href="../../10/24/inferring_probabilities_a_second_example_of_bayesian_calculations.html#bayes-second-example"><em>Inferring probabilities, a second example of Bayesian calculations</em></a>. In particular, instead of
considering a discrete set of candidate probabilities, I&#8217;ll consider all
(continuous) values between <span class="math">\(0\)</span> and <span class="math">\(1\)</span>.  This means our prior (and
posterior) will now be a <a class="reference external" href="http://en.wikipedia.org/wiki/Probability_density_function">probability density function</a> (pdf) instead of a
<a class="reference external" href="http://en.wikipedia.org/wiki/Probability_mass_function">probability mass function</a> (pmf).  More specifically, I&#8217;ll use the
<a class="reference external" href="http://en.wikipedia.org/wiki/Beta_distribution">Beta Distribution</a> for this example.</p>
<div id="more"> </div><p>In the post <a class="reference internal" href="../../10/24/inferring_probabilities_a_second_example_of_bayesian_calculations.html#bayes-second-example"><em>Inferring probabilities, a second example of Bayesian calculations</em></a> I considered inference of
<span class="math">\(p_{0}\)</span>, the probability for a zero, from a data series:</p>
<div class="math">
\[D = 0000110001\]</div>
<p><em>I&#8217;ll tackle the same question in this post</em> using a different prior for
<span class="math">\(p_{0}\)</span>, one that allows for continuous values between <span class="math">\(0\)</span> and
<span class="math">\(1\)</span> instead of a discrete set of candidates.  This makes the math a bit
more difficult, requiring integrals instead of sums, but the basic ideas are
the same.</p>
<p>This is one in a series of posts on Bayesian methods, starting from the basics
and increasing in difficulty:</p>
<ul class="simple">
<li><a class="reference internal" href="../../08/26/joint_conditional_and_marginal_probabilities.html#joint-conditional-and-marginal-probabilities"><em>Joint, conditional and marginal probabilities</em></a></li>
<li><a class="reference internal" href="../../09/11/medical_tests_a_first_example_of_bayesian_calculations.html#bayes-medical-tests"><em>Medical tests, a first example of Bayesian calculations</em></a></li>
<li><a class="reference internal" href="../../10/24/inferring_probabilities_a_second_example_of_bayesian_calculations.html#bayes-second-example"><em>Inferring probabilities, a second example of Bayesian calculations</em></a></li>
</ul>
<p>If the following is unfamiliar or difficult try consulting one or more of the
above posts for some more basic, introductory material.</p>
<div class="section" id="likelihood">
<h2>Likelihood</h2>
<p>The starting point for our inference problem is the <em>likelihood</em> &#8211; the
probability of the observed data series, written like I know the value of
<span class="math">\(p_{0}\)</span> (see the <a class="reference internal" href="../../10/24/inferring_probabilities_a_second_example_of_bayesian_calculations.html#bayes-second-example"><em>Inferring probabilities, a second example of Bayesian calculations</em></a> post for more details, I&#8217;ll
be brief here):</p>
<div class="math">
\[P(D=0000110001 \vert p_{0} ) = p_{0}^{7} \times (1-p_{0})^{3}\]</div>
<p>To be clear, we could plug in <span class="math">\(p_{0}=0.6\)</span>, and find the probability of
the specified data series given that value for the unknown probability. A more
general form for the likelihood, not being specific about the data series
considered, is</p>
<div class="math">
\[P(D \vert p_{0} ) = p_{0}^{n{0}} \times (1-p_{0})^{n_{1}}\]</div>
<p>where <span class="math">\(n_{0}\)</span> is the number of zeros and <span class="math">\(n_{1}\)</span> is the number of
ones in whatever data series <span class="math">\(D\)</span> is considered.</p>
</div>
<div class="section" id="prior-the-beta-distribution">
<h2>Prior &#8211; The Beta Distribution</h2>
<p>The new material, relative to the previous post on this topic, starts here. We
use the <a class="reference external" href="http://en.wikipedia.org/wiki/Beta_distribution">Beta Distribution</a> to represent our prior assumptions/information.
The mathematical form is:</p>
<div class="math">
\[P(p_{0} \vert \alpha_{0}, \alpha_{1} )  =
  \frac{
    \Gamma(\alpha_{0} + \alpha_{1})
    }{
    \Gamma(\alpha_{0}) \Gamma(\alpha_{1})
    } \,
p_{0}^{\alpha_{0}-1} \, (1-p_{0})^{\alpha_{1}-1}\]</div>
<p>where <span class="math">\(\alpha_{0}\)</span> and <span class="math">\(\alpha_{1}\)</span> are hyper-parameters that we
have to set to reflect our prior assumptions/information about the value of
<span class="math">\(p_{0}\)</span>.  It is useful to understand some of the properties of the
<a class="reference external" href="http://en.wikipedia.org/wiki/Beta_distribution">Beta Distribution</a>, so let&#8217;s go over some:</p>
<ul class="simple">
<li><strong>The pdf is normalized.</strong> This means if we integrate <span class="math">\(p_{0}\)</span> from
<span class="math">\(0\)</span> to <span class="math">\(1\)</span> we get one:</li>
</ul>
<div class="math">
\[\int_{0}^{1} \, dp_{0} \, P(p_{0} \vert \alpha_{0}, \alpha{1}) = 1\]</div>
<p>This is true because of the following relationship:</p>
<div class="math">
\[\int_{0}^{1} \, dp_{0} \, p_{0}^{\alpha_{0}-1} \, (1-p_{0})^{\alpha_{1}-1}
=
\frac{\Gamma(\alpha_{0}) \Gamma(\alpha_{1})
     }{\Gamma(\alpha_{0} + \alpha_{1})}\]</div>
<p>The above integral produces the <a class="reference external" href="http://en.wikipedia.org/wiki/Beta_function">Beta function</a> (the relation is also called
the Euler integral).  For our purposes, the most import information is the
<a class="reference external" href="http://en.wikipedia.org/wiki/Beta_distribution">Beta Distribution</a> is normalized on the <span class="math">\(0\)</span> to <span class="math">\(1\)</span> interval, as
necessary for <span class="math">\(p_{0}\)</span>.</p>
<ul>
<li><p class="first"><strong>Prior assumptions/information can be reflected by setting
hyper-parameters</strong>.  The hyper-parameters <span class="math">\(\alpha_{0}\)</span> and
<span class="math">\(\alpha_{1}\)</span> affect the shape of the pdf, enabling a flexible encoding
of prior information.</p>
<p>For example, no (a priori) preferred values of <span class="math">\(p_{0}\)</span> can be reflected
by using <span class="math">\(\alpha_{0}=1\)</span>, <span class="math">\(\alpha_{1}=1\)</span>. This pdf looks like</p>
</li>
</ul>
<a class="reference internal image-reference" href="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure2_1.png"><img alt="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure2_1.png" src="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure2_1.png" style="width: 15cm;" /></a>
<p>and reflects the fact that a value of <span class="math">\(p_{0}\)</span> between 0.0 and 0.2 has
the same prior probability as value between 0.6 and 0.8. To find these
probabilities, we&#8217;d have to integrate the Beta pdf over the appropriate range.
For example, we can look at the cumulative density function</p>
<a class="reference internal image-reference" href="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure4_1.png"><img alt="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure4_1.png" src="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure4_1.png" style="width: 15cm;" /></a>
<p>Another prior could assign <span class="math">\(\alpha_{0}=5\)</span>, <span class="math">\(\alpha_{1}=5\)</span>, which
prefers values near <span class="math">\(p_{0}=1/2\)</span> and looks like</p>
<a class="reference internal image-reference" href="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure5_1.png"><img alt="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure5_1.png" src="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure5_1.png" style="width: 15cm;" /></a>
<p>The CDS for this prior looks different:</p>
<a class="reference internal image-reference" href="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure6_1.png"><img alt="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure6_1.png" src="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure6_1.png" style="width: 15cm;" /></a>
<ul class="simple">
<li><strong>The Beta Distribution is a conjugate prior for this problem.</strong> This means
that the posterior will have the same mathematical form as the prior (it will
also be a <a class="reference external" href="http://en.wikipedia.org/wiki/Beta_distribution">Beta Distribution</a>) with
updated hyper-parameters.  This mathematical &#8216;resonance&#8217; is really nice and
let&#8217;s us do full Bayesian inference without MCMC.</li>
</ul>
<p>Some final notes before moving on to Bayes&#8217; Theorem and the posterior for this
problem:</p>
<ul class="simple">
<li>I&#8217;ve used different notation for the Beta: <span class="math">\((\alpha_{0}, \alpha_{1})\)</span>
instead of the usual <span class="math">\((\alpha, \beta)\)</span>.  As I&#8217;ll discuss further below,
this notation makes it easier to compare the hyper-parameters with
<em>fake counts</em> and relate the values with data <span class="math">\(n_{0}\)</span> and <span class="math">\(n_{1}\)</span>.</li>
<li>It useful to use the <em>mean</em> as a summary of the prior settings. For the
Beta pdf, the mean is</li>
</ul>
<div class="math">
\[\begin{split}\begin{array}{ll}
  \mathbf{E}_{prior}[p_{0}] &amp; = &amp; \int_{0}^{1} \, dp_{0} \, p_{0} \,
                                  P(p_{0} \vert \alpha_{0}, \alpha_{1}) \\
  &amp; = &amp; \frac{\alpha_{0}}{\alpha_{0}+\alpha_{1}}
\end{array}\end{split}\]</div>
<p>Many other properties (higher moments, variance, etc) can be calculated&#8211; see
<a class="reference external" href="http://en.wikipedia.org/wiki/Beta_distribution">Beta Distribution</a> for more options.  Also checkout this nice post on
<a class="reference external" href="http://sumsar.net/blog/2014/10/probable-points-and-credible-intervals-part-one/">Probable Points and Credible Intervals</a> for ideas on how to summarize a
posterior distribution (also relevant to priors).</p>
</div>
<div class="section" id="bayes-theorem-and-the-posterior">
<h2>Bayes&#8217; Theorem and the Posterior</h2>
<p>Our final goal is the posterior probability density function, combining the
likelihood and the prior to make an updated reflection of our
knowledge of <span class="math">\(p_{0}\)</span> after considering data. The posterior pdf has the
form (in this case):</p>
<div class="math">
\[P(p_{0} \vert D, \alpha_{0}, \alpha_{1})\]</div>
<p>In words, this is <em>the probability density for</em> <span class="math">\(p_{0}\)</span> <em>given data
series</em> <span class="math">\(D\)</span> <em>and prior assumptions, reflected by the Beta pdf with
hyper-parameters</em> <span class="math">\((\alpha_{0}, \alpha_{1})\)</span>.</p>
<p>In this setting Bayes&#8217; Theorem takes the form:</p>
<div class="math">
\[\color{blue}{P(p_{0} \vert D, \alpha_{0}, \alpha_{1})}
= \frac{P(D \vert p_{0})
  \color{red}{P(p_{0} \vert \alpha_{0}, \alpha_{1})}
  }{
  \int_{0}^{1} \, d\hat{p}_{0} \,
  P(D \vert \hat{p}_{0})
  \color{red}{P(\hat{p}_{0} \vert \alpha_{0}, \alpha_{1})}
  }\]</div>
<p>where the posterior
<span class="math">\(\color{blue}{P(p_{0} \vert D, \alpha_{0}, \alpha_{1})}\)</span> is blue, the
likelihood <span class="math">\(P(D \vert \hat{p}_{0})\)</span> is black, and the prior
<span class="math">\(\color{red}{P(p_{0} \vert \alpha_{0}, \alpha_{1})}\)</span> is red.
Notice that the normalizing <em>marginal likelihood</em> or <em>evidence</em> (denominator in
the above equation) is now an integral.  This is the price of using continuous
values for <span class="math">\(p_{0}\)</span>&#8211; you should compare this with Bayes&#8217; Theorem in the
<a class="reference internal" href="../../10/24/inferring_probabilities_a_second_example_of_bayesian_calculations.html#bayes-second-example"><em>Inferring probabilities, a second example of Bayesian calculations</em></a> post.</p>
<p>As always, try to think about Bayes&#8217; Theorem as information about <span class="math">\(p_{0}\)</span>
being updated from <strong>assumptions</strong> (<span class="math">\(\alpha_{0}, \alpha_{1}\)</span>)
to <strong>assumptions + data</strong> (<span class="math">\(D, \alpha_{0}, \alpha_{1}\)</span>):</p>
<div class="math">
\[\color{red}{P(p_{0} \vert \alpha_{0}, \alpha_{1})}
\rightarrow
\color{blue}{P(p_{0} \vert D, \alpha_{0}, \alpha_{1})}\]</div>
<p>To get the posterior pdf, we have to do the integral in the denominator of
Bayes&#8217; Theorem.  In this case, the calculation is possible, using the
properties of the <a class="reference external" href="http://en.wikipedia.org/wiki/Beta_distribution">Beta Distribution</a>.  The integral goes as follows:</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
P(D \vert \alpha_{0}, \alpha_{1})
&amp; = &amp;
  \int_{0}^{1} \, d\hat{p}_{0} \,
  P(D \vert \hat{p}_{0})
  P(\hat{p}_{0} \vert \alpha_{0}, \alpha_{1}) \\
&amp; &amp; \\
&amp; = &amp;  \int_{0}^{1} \, d\hat{p}_{0} \,
       \hat{p}_{0}^{n_{0}} \, (1-\hat{p}_{0})^{n_{1}} \\
&amp; \times &amp;
  \frac{ \Gamma(\alpha_{0} + \alpha_{1})
    }{
    \Gamma(\alpha_{0}) \Gamma(\alpha_{1}) }
    \hat{p}_{0}^{\alpha_{0}-1} (1-\hat{p}_{0})^{\alpha_{1}-1} \\
&amp; &amp; \\
&amp; = &amp;
  \frac{ \Gamma(\alpha_{0} + \alpha_{1})
   }{
   \Gamma(\alpha_{0}) \Gamma(\alpha_{1}) } \\
&amp; \times &amp;
   \int_{0}^{1} \, d\hat{p}_{0} \,
   \hat{p}_{0}^{\alpha_{0}+n_{0}-1} \, (1-\hat{p}_{0})^{\alpha_{1}+n_{1}-1}
\end{array}\end{split}\]</div>
<p>The integral on the last line defines a <a class="reference external" href="http://en.wikipedia.org/wiki/Beta_function">Beta Function</a>, as discussed in
the section on the prior, and has a known result:</p>
<div class="math">
\[\int_{0}^{1} \, dp_{0} \, p_{0}^{\alpha_{0}+n_{0}-1}
\, (1-p_{0})^{\alpha_{1}+n_{1}-1}
=
\frac{\Gamma(\alpha_{0}+n_{0}) \Gamma(\alpha_{1}+n_{1})
     }{\Gamma(\alpha_{0} + \alpha_{1} + n_{0} + n_{1})}\]</div>
<p>This means the denominator, also called the <strong>marginal likelihood</strong> or
<strong>evidence</strong>, is equal to:</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
P(D \vert \alpha_{0}, \alpha_{1})
&amp; = &amp;
  \frac{ \Gamma(\alpha_{0} + \alpha_{1})
   }{
   \Gamma(\alpha_{0}) \Gamma(\alpha_{1}) } \\
&amp; \times &amp;
  \frac{\Gamma(\alpha_{0}+n_{0}) \Gamma(\alpha_{1}+n_{1})
   }{
   \Gamma(\alpha_{0} + \alpha_{1} + n_{0} + n_{1})}
\end{array}\end{split}\]</div>
<p>If we plug all of this back into Bayes&#8217; Theorem we get another <a class="reference external" href="http://en.wikipedia.org/wiki/Beta_distribution">Beta
Distribution</a> for the <strong>posterior pdf</strong>, as promised above:</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
P(p_{0} \vert D, \alpha_{0}, \alpha_{1} )
&amp; =  &amp;
  \frac{
    \Gamma(\alpha_{0} + \alpha_{1} + n_{0} + n_{1})
    }{
    \Gamma(\alpha_{0}+n_{0}) \Gamma(\alpha_{1}+n_{1})
    } \\
&amp; \times &amp;
  p_{0}^{\alpha_{0}+n_{0}-1} \, (1-p_{0})^{\alpha_{1}+n_{1}-1}
\end{array}\end{split}\]</div>
<p>Again, we obtain this result because the <a class="reference external" href="http://en.wikipedia.org/wiki/Beta_distribution">Beta Distribution</a> is a conjugate
prior for the <a class="reference external" href="http://en.wikipedia.org/wiki/Bernoulli_process">Bernoulli Process</a> likelihood that we are considering.  Notice
that the hyper-parameters from the prior have been updated by count data</p>
<div class="math">
\[(\alpha_{0}, \alpha_{1})
\rightarrow
(\alpha_{0}+n_{0}, \alpha_{1}+n_{1})\]</div>
<p>This is exactly as one might expect without doing all of the math. In any case,
before moving to implementing this in Python, a couple of notes:</p>
<ul class="simple">
<li>The posterior pdf is normalized on the <span class="math">\(0\)</span> to <span class="math">\(1\)</span> interval, just
as we need for inferring a probability like <span class="math">\(p_{0}\)</span>.</li>
<li>The posterior mean, a way to give a point estimate of our inference is</li>
</ul>
<div class="math">
\[\begin{split}\begin{array}{ll}
  \mathbf{E}_{post}[p_{0}] &amp; = &amp; \int_{0}^{1} \, dp_{0} \, p_{0} \,
                           P(p_{0} \vert D, \alpha_{0}, \alpha_{1}) \\
  &amp; = &amp; \frac{\alpha_{0}+n_{0}}{\alpha_{0}+\alpha_{1}+n_{0}+n_{1}}
\end{array}\end{split}\]</div>
</div>
<div class="section" id="inference-code-in-python">
<h2>Inference code in Python</h2>
<p><strong>Note:</strong> code available as <span class="code docutils literal"><span class="pre">ex003_bayes.py</span></span> at
<a class="reference external" href="https://github.com/cstrelioff/chrisstrelioffws-sandbox-examples">github examples repository</a>.</p>
<p>Let&#8217;s do some Python.  First, we do some import of packages that we will use
to calculate and plot prior, likelihood and posterior.  Notice that
<span class="code docutils literal"><span class="pre">scipy.stats</span></span> has a <span class="code docutils literal"><span class="pre">beta</span></span> class that we will use for the prior and
posterior pdfs.  Also, we use <span class="code docutils literal"><span class="pre">matplotlib</span></span> and the new styles, ggplot in
this case, to create some nice plots with minimal tweaking.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span><span class="p">,</span> <span class="n">print_function</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">beta</span>

<span class="c"># use matplotlib style sheet</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s">&#39;ggplot&#39;</span><span class="p">)</span>
<span class="k">except</span><span class="p">:</span>
    <span class="c"># version of matplotlib might not be recent</span>
    <span class="k">pass</span>
</pre></div>
</div>
<p><strong>Likelihood</strong></p>
<p>The likelihood is exactly the same as for the previous example&#8211; see
<a class="reference internal" href="../../10/24/inferring_probabilities_a_second_example_of_bayesian_calculations.html#bayes-second-example"><em>Inferring probabilities, a second example of Bayesian calculations</em></a>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">class</span> <span class="nc">likelihood</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Likelihood for binary data.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">counts</span> <span class="o">=</span> <span class="p">{</span><span class="n">s</span><span class="p">:</span><span class="mi">0</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="p">[</span><span class="s">&#39;0&#39;</span><span class="p">,</span> <span class="s">&#39;1&#39;</span><span class="p">]}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_process_data</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_process_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Process data.&quot;&quot;&quot;</span>
        <span class="n">temp</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="p">[</span><span class="s">&#39;0&#39;</span><span class="p">,</span> <span class="s">&#39;1&#39;</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">counts</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">temp</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">counts</span><span class="o">.</span><span class="n">values</span><span class="p">()):</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s">&quot;Passed data is not all 0`s and 1`s!&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_process_probabilities</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Process probabilities.&quot;&quot;&quot;</span>
        <span class="n">n0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">counts</span><span class="p">[</span><span class="s">&#39;0&#39;</span><span class="p">]</span>
        <span class="n">n1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">counts</span><span class="p">[</span><span class="s">&#39;1&#39;</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">p0</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">p0</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c"># typical case</span>
            <span class="n">logpr_data</span> <span class="o">=</span> <span class="n">n0</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p0</span><span class="p">)</span> <span class="o">+</span> \
                         <span class="n">n1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1.</span><span class="o">-</span><span class="n">p0</span><span class="p">)</span>
            <span class="n">pr_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logpr_data</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">p0</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">n0</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c"># p0 can&#39;t be 0 if n0 is not 0</span>
            <span class="n">logpr_data</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
            <span class="n">pr_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logpr_data</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">p0</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">n0</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c"># data consistent with p0=0</span>
            <span class="n">logpr_data</span> <span class="o">=</span> <span class="n">n1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1.</span><span class="o">-</span><span class="n">p0</span><span class="p">)</span>
            <span class="n">pr_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logpr_data</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">p0</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">n1</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c"># p0 can&#39;t be 1 if n1 is not 0</span>
            <span class="n">logpr_data</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
            <span class="n">pr_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logpr_data</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">p0</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">n1</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c"># data consistent with p0=1</span>
            <span class="n">logpr_data</span> <span class="o">=</span> <span class="n">n0</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p0</span><span class="p">)</span>
            <span class="n">pr_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logpr_data</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">pr_data</span><span class="p">,</span> <span class="n">logpr_data</span>

    <span class="k">def</span> <span class="nf">prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Get probability of data.&quot;&quot;&quot;</span>
        <span class="n">pr_data</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_probabilities</span><span class="p">(</span><span class="n">p0</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">pr_data</span>

    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Get log of probability of data.&quot;&quot;&quot;</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">logpr_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_probabilities</span><span class="p">(</span><span class="n">p0</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">logpr_data</span>
</pre></div>
</div>
<p><strong>Prior</strong></p>
<p>Our prior class will basically be a wrapper around <span class="code docutils literal"><span class="pre">scipy.stats.beta</span></span>
with a plotting method.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">class</span> <span class="nc">prior</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha0</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha1</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Beta prior for binary data.&quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">a0</span> <span class="o">=</span> <span class="n">alpha0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a1</span> <span class="o">=</span> <span class="n">alpha1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p0rv</span> <span class="o">=</span> <span class="n">beta</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">a1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">interval</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prob</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;End points for region of pdf containing `prob` of the</span>
<span class="sd">        pdf.</span>

<span class="sd">        Ex: interval(0.95)</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">p0rv</span><span class="o">.</span><span class="n">interval</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">mean</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns prior mean.&quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">p0rv</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">pdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Probability density at p0.&quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">p0rv</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">p0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;A plot showing mean and 95% credible interval.&quot;&quot;&quot;</span>

        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>

        <span class="c"># get prior mean p0</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="c"># get low/high pts containg 95% probability</span>
        <span class="n">low_p0</span><span class="p">,</span> <span class="n">high_p0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">interval</span><span class="p">(</span><span class="mf">0.95</span><span class="p">)</span>
        <span class="n">x_prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">low_p0</span><span class="p">,</span> <span class="n">high_p0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>

        <span class="c"># plot pdf</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s">&#39;r-&#39;</span><span class="p">)</span>

        <span class="c"># fill 95% region</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_prob</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_prob</span><span class="p">),</span>
                        <span class="n">color</span><span class="o">=</span><span class="s">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="s">&#39;0.2&#39;</span> <span class="p">)</span>

        <span class="c"># mean</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">stem</span><span class="p">([</span><span class="n">mean</span><span class="p">],</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">mean</span><span class="p">)],</span> <span class="n">linefmt</span><span class="o">=</span><span class="s">&#39;r-&#39;</span><span class="p">,</span>
                <span class="n">markerfmt</span><span class="o">=</span><span class="s">&#39;ro&#39;</span><span class="p">,</span> <span class="n">basefmt</span><span class="o">=</span><span class="s">&#39;w-&#39;</span><span class="p">)</span>

        <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">&#39;Probability of Zero&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">&#39;Prior PDF&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Let&#8217;s plot some Beta pdfs with a range of parameters using the new code. First,
the uniform prior</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">pri</span> <span class="o">=</span> <span class="n">prior</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">&quot;Prior mean: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pri</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>
<span class="n">cred_int</span> <span class="o">=</span> <span class="n">pri</span><span class="o">.</span><span class="n">interval</span><span class="p">(</span><span class="mf">0.95</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">&quot;95% CI: {} -- {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cred_int</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cred_int</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">pri</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre>Prior mean: 0.5
95% CI: 0.025 -- 0.975
</pre></div>
</div>
<a class="reference internal image-reference" href="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure10_1.png"><img alt="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure10_1.png" src="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure10_1.png" style="width: 15cm;" /></a>
<p>Second, a prior with mean <span class="math">\(\mathbf{E}_{prior} = 2/(2+5) \approx 0.29\)</span>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">pri</span> <span class="o">=</span> <span class="n">prior</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">&quot;Prior mean: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pri</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>
<span class="n">cred_int</span> <span class="o">=</span> <span class="n">pri</span><span class="o">.</span><span class="n">interval</span><span class="p">(</span><span class="mf">0.95</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">&quot;95% CI: {} -- {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cred_int</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cred_int</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">pri</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre>Prior mean: 0.285714285714
95% CI: 0.0432718682927 -- 0.641234578998
</pre></div>
</div>
<a class="reference internal image-reference" href="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure11_1.png"><img alt="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure11_1.png" src="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure11_1.png" style="width: 15cm;" /></a>
<p>It&#8217;s useful to get a feel for the mean and uncertainty of prior assumptions, as
reflected by the hyper-parameters&#8211; try out some other values to build an
intuition.</p>
<p><strong>Posterior</strong></p>
<p>Finally, we build the class for the posterior.  As you might expect, I&#8217;ll take
data and a prior as arguments and extract the parameters needed for the
posterior from these elements.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">class</span> <span class="nc">posterior</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">prior</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;The posterior.</span>

<span class="sd">        data: a data sample as list</span>
<span class="sd">        prior: an instance of the beta prior class</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span> <span class="o">=</span> <span class="n">likelihood</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prior</span> <span class="o">=</span> <span class="n">prior</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_process_posterior</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_process_posterior</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Process the posterior using passed data and prior.&quot;&quot;&quot;</span>

        <span class="c"># extract n0, n1, a0, a1 from likelihood and prior</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span><span class="o">.</span><span class="n">counts</span><span class="p">[</span><span class="s">&#39;0&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span><span class="o">.</span><span class="n">counts</span><span class="p">[</span><span class="s">&#39;1&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">a0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">a1</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">p0rv</span> <span class="o">=</span> <span class="n">beta</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a0</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">n0</span><span class="p">,</span>
                         <span class="bp">self</span><span class="o">.</span><span class="n">a1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">n1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">interval</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prob</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;End points for region of pdf containing `prob` of the</span>
<span class="sd">        pdf.</span>

<span class="sd">        Ex: interval(0.95)</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">p0rv</span><span class="o">.</span><span class="n">interval</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">mean</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns posterior mean.&quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">p0rv</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">pdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Probability density at p0.&quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">p0rv</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">p0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;A plot showing prior, likelihood and posterior.&quot;&quot;&quot;</span>

        <span class="n">f</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>

        <span class="c">## Prior</span>
        <span class="c"># get prior mean p0</span>
        <span class="n">pri_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="c"># get low/high pts containg 95% probability</span>
        <span class="n">pri_low_p0</span><span class="p">,</span> <span class="n">pri_high_p0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">interval</span><span class="p">(</span><span class="mf">0.95</span><span class="p">)</span>
        <span class="n">pri_x_prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">pri_low_p0</span><span class="p">,</span> <span class="n">pri_high_p0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>

        <span class="c"># plot pdf</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s">&#39;r-&#39;</span><span class="p">)</span>

        <span class="c"># fill 95% region</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">pri_x_prob</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">pri_x_prob</span><span class="p">),</span>
                           <span class="n">color</span><span class="o">=</span><span class="s">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="s">&#39;0.2&#39;</span> <span class="p">)</span>

        <span class="c"># mean</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">stem</span><span class="p">([</span><span class="n">pri_mean</span><span class="p">],</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">pri_mean</span><span class="p">)],</span>
                   <span class="n">linefmt</span><span class="o">=</span><span class="s">&#39;r-&#39;</span><span class="p">,</span> <span class="n">markerfmt</span><span class="o">=</span><span class="s">&#39;ro&#39;</span><span class="p">,</span>
                   <span class="n">basefmt</span><span class="o">=</span><span class="s">&#39;w-&#39;</span><span class="p">)</span>

        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">&#39;Prior PDF&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>

        <span class="c">## Likelihood</span>
        <span class="c"># plot likelihood</span>
        <span class="n">lik</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="n">xi</span><span class="p">)</span> <span class="k">for</span> <span class="n">xi</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lik</span><span class="p">,</span> <span class="s">&#39;k-&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">&#39;Likelihood&#39;</span><span class="p">)</span>

        <span class="c">## Posterior</span>
        <span class="c"># get posterior mean p0</span>
        <span class="n">post_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="c"># get low/high pts containg 95% probability</span>
        <span class="n">post_low_p0</span><span class="p">,</span> <span class="n">post_high_p0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">interval</span><span class="p">(</span><span class="mf">0.95</span><span class="p">)</span>
        <span class="n">post_x_prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">post_low_p0</span><span class="p">,</span> <span class="n">post_high_p0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>

        <span class="c"># plot pdf</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s">&#39;b-&#39;</span><span class="p">)</span>

        <span class="c"># fill 95% region</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">post_x_prob</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">post_x_prob</span><span class="p">),</span>
                           <span class="n">color</span><span class="o">=</span><span class="s">&#39;blue&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="s">&#39;0.2&#39;</span> <span class="p">)</span>

        <span class="c"># mean</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">stem</span><span class="p">([</span><span class="n">post_mean</span><span class="p">],</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">post_mean</span><span class="p">)],</span>
                   <span class="n">linefmt</span><span class="o">=</span><span class="s">&#39;b-&#39;</span><span class="p">,</span> <span class="n">markerfmt</span><span class="o">=</span><span class="s">&#39;bo&#39;</span><span class="p">,</span>
                   <span class="n">basefmt</span><span class="o">=</span><span class="s">&#39;w-&#39;</span><span class="p">)</span>

        <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">&#39;Probability of Zero&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">&#39;Posterior PDF&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>That&#8217;s it with the base code, let do some examples.</p>
</div>
<div class="section" id="examples">
<h2>Examples</h2>
<p>Let&#8217;s do an example</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># data</span>
<span class="n">data1</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>

<span class="c"># prior</span>
<span class="n">prior1</span> <span class="o">=</span> <span class="n">prior</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c"># posterior</span>
<span class="n">post1</span> <span class="o">=</span> <span class="n">posterior</span><span class="p">(</span><span class="n">data1</span><span class="p">,</span> <span class="n">prior1</span><span class="p">)</span>
<span class="n">post1</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure13_1.png"><img alt="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure13_1.png" src="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure13_1.png" style="width: 15cm;" /></a>
<p>Next, an example where the prior is not uniform and produced a biased picture
due to this setting (again, there should be a good reason for this prior
setting):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># prior</span>
<span class="n">prior2</span> <span class="o">=</span> <span class="n">prior</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>

<span class="c"># posterior</span>
<span class="n">post2</span> <span class="o">=</span> <span class="n">posterior</span><span class="p">(</span><span class="n">data1</span><span class="p">,</span> <span class="n">prior2</span><span class="p">)</span>
<span class="n">post2</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure14_1.png"><img alt="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure14_1.png" src="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure14_1.png" style="width: 15cm;" /></a>
<p>And a final example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># set probability of 0</span>
<span class="n">p0</span> <span class="o">=</span> <span class="mf">0.23</span>
<span class="c"># set rng seed to 42</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="c"># generate data</span>
<span class="n">data2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="mi">500</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="p">[</span><span class="n">p0</span><span class="p">,</span> <span class="mf">1.</span><span class="o">-</span><span class="n">p0</span><span class="p">])</span>

<span class="c"># prior</span>
<span class="n">prior3</span> <span class="o">=</span> <span class="n">prior</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="c"># posterior</span>
<span class="n">post3</span> <span class="o">=</span> <span class="n">posterior</span><span class="p">(</span><span class="n">data2</span><span class="p">,</span> <span class="n">prior3</span><span class="p">)</span>
<span class="n">post3</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure15_1.png"><img alt="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure15_1.png" src="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure15_1.png" style="width: 15cm;" /></a>
<p>a bad prior, but more data (using the data from above example)</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># prior</span>
<span class="n">prior4</span> <span class="o">=</span> <span class="n">prior</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">)</span>

<span class="c"># posterior</span>
<span class="n">post4</span> <span class="o">=</span> <span class="n">posterior</span><span class="p">(</span><span class="n">data2</span><span class="p">,</span> <span class="n">prior4</span><span class="p">)</span>
<span class="n">post4</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure16_1.png"><img alt="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure16_1.png" src="../../../_images/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations_figure16_1.png" style="width: 15cm;" /></a>
</div>
</div>

    <div class="postmeta">
        <div class="author">
            <span>Posted by Chris Strelioff</span>
        </div>
        
        <div class="tags">
            <span>
                Tags:
                <a href="../../../tags/joint_probability.html">joint probability</a>, <a href="../../../tags/conditional_probability.html">conditional probability</a>, <a href="../../../tags/marginal_probability.html">marginal probability</a>, <a href="../../../tags/bayesian.html">Bayesian</a>, <a href="../../../tags/python.html">python</a>, <a href="../../../tags/beta.html">Beta</a></span>
        </div>
        </div><ul class="related clearfix">
            <li class="left"></li>
            <li class="right"><a href="../10/installing_essentia_for_audio_feature_extraction.html">Installing essentia for audio feature extraction</a> &raquo; </li>
        </ul><div id="disqus_thread"></div><script type="text/javascript">    var disqus_shortname = "chrissandbox";    var disqus_identifier = "2014/12/11/inferring_probabilities_with_a_beta_prior_a_third_example_of_bayesian_calculations";    disqus_thread();</script><noscript>Please enable JavaScript to view the    <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></article><aside class="sidebar"><section><div class="widget" id="searchbox" role="search">
    <h1><a href="#searchbox">Search</a></h1>
    <form action="../../../search.html" method="get">
        <input type="text" name="q" />
        <button type="submit"><span class="fa fa-search"></span></button>
    </form>
</div></section><section><div class="widget">
  <h1>Pages</h1>
  <ul>
  <li>
      <a href="../../../index.html">Home</a>
    </li>
  </ul>
</div>
</section><section><div class="widget">
    <h1>Recent Posts</h1>
    <ul><li>
            <a href="#">Inferring probabilities with a Beta prior, a third example of Bayesian calculations</a>
        </li><li>
            <a href="../10/installing_essentia_for_audio_feature_extraction.html">Installing essentia for audio feature extraction</a>
        </li><li>
            <a href="../../11/13/getting_started_with_latent_dirichlet_allocation_in_python.html">Getting started with Latent Dirichlet Allocation in Python</a>
        </li><li>
            <a href="../../10/24/inferring_probabilities_a_second_example_of_bayesian_calculations.html">Inferring probabilities, a second example of Bayesian calculations</a>
        </li><li>
            <a href="../../09/16/python_3_4_on_ubuntu_14_04_using_virtual_environments.html">Python 3.4 on Ubuntu 14.04 using virtual environments</a>
        </li><li>
            <a href="../../09/11/medical_tests_a_first_example_of_bayesian_calculations.html">Medical tests, a first example of Bayesian calculations</a>
        </li><li>
            <a href="../../09/04/virtualenv_and_virtualenvwrapper_on_ubuntu_14_04.html">virtualenv and virtualenvwrapper on Ubuntu 14.04</a>
        </li><li>
            <a href="../../09/02/installing_virtualbox_on_ubuntu_14_04.html">Installing virtualbox on Ubuntu 14.04</a>
        </li><li>
            <a href="../../08/26/joint_conditional_and_marginal_probabilities.html">Joint, conditional and marginal probabilities</a>
        </li><li>
            <a href="../../08/16/garmin_forerunner_and_ubuntu_14_04.html">Garmin forerunner and Ubuntu 14.04</a>
        </li></ul>
</div>
</section><section><div class="widget">
    <h1>Tags</h1><a href="../../../tags/audio_features.html">audio features</a> (1), <a href="../../../tags/bayesian.html">Bayesian</a> (5), <a href="../../../tags/beta.html">Beta</a> (1), <a href="../../../tags/blog_setup.html">blog setup</a> (1), <a href="../../../tags/bootstrap.html">bootstrap</a> (1), <a href="../../../tags/bottleneck.html">bottleneck</a> (1), <a href="../../../tags/c.html">c++</a> (1), <a href="../../../tags/caret.html">caret</a> (1), <a href="../../../tags/cmpy.html">cmpy</a> (1), <a href="../../../tags/conditional_probability.html">conditional probability</a> (4), <a href="../../../tags/coursera.html">coursera</a> (1), <a href="../../../tags/coursera_intro_to_data_science.html">coursera intro to data science</a> (3), <a href="../../../tags/cython.html">cython</a> (1), <a href="../../../tags/dsp.html">dsp</a> (1), <a href="../../../tags/e1071.html">e1071</a> (1), <a href="../../../tags/essentia.html">essentia</a> (1), <a href="../../../tags/garmin.html">garmin</a> (1), <a href="../../../tags/ggplot2.html">ggplot2</a> (1), <a href="../../../tags/git.html">git</a> (1), <a href="../../../tags/gnuplot.html">gnuplot</a> (1), <a href="../../../tags/graphs.html">graphs</a> (1), <a href="../../../tags/igraph.html">igraph</a> (1), <a href="../../../tags/ipython.html">ipython</a> (1), <a href="../../../tags/joint_probability.html">joint probability</a> (4), <a href="../../../tags/latex.html">LaTeX</a> (1), <a href="../../../tags/lda.html">LDA</a> (1), <a href="../../../tags/machine_learning.html">machine learning</a> (1), <a href="../../../tags/marginal_probability.html">marginal probability</a> (4), <a href="../../../tags/matplotlib.html">matplotlib</a> (1), <a href="../../../tags/mir.html">mir</a> (1), <a href="../../../tags/my_python_setup.html">my python setup</a> (6), <a href="../../../tags/my_ubuntu_setup.html">my ubuntu setup</a> (10), <a href="../../../tags/networks.html">networks</a> (1), <a href="../../../tags/networkx.html">networkx</a> (1), <a href="../../../tags/numexpr.html">numexpr</a> (1), <a href="../../../tags/numpy.html">numpy</a> (1), <a href="../../../tags/octave.html">octave</a> (1), <a href="../../../tags/openpyxl.html">openpyxl</a> (1), <a href="../../../tags/pandas.html">pandas</a> (1), <a href="../../../tags/patsy.html">patsy</a> (1), <a href="../../../tags/pip.html">pip</a> (1), <a href="../../../tags/pweave.html">pweave</a> (1), <a href="../../../tags/pygraphviz.html">pygraphviz</a> (1), <a href="../../../tags/pymc.html">pymc</a> (1), <a href="../../../tags/python.html">Python</a> (1), <a href="../../../tags/python.html">python</a> (5), <a href="../../../tags/python_2_7.html">python 2.7</a> (5), <a href="../../../tags/python_3_4.html">python 3.4</a> (1), <a href="../../../tags/pyyaml.html">pyyaml</a> (1), <a href="../../../tags/r.html">R</a> (1), <a href="../../../tags/randomforest.html">randomForest</a> (1), <a href="../../../tags/restview.html">restview</a> (1), <a href="../../../tags/resume.html">resume</a> (1), <a href="../../../tags/rpart.html">rpart</a> (1), <a href="../../../tags/running.html">running</a> (1), <a href="../../../tags/scikit_learn.html">scikit-learn</a> (1), <a href="../../../tags/scipy.html">scipy</a> (1), <a href="../../../tags/screen.html">screen</a> (1), <a href="../../../tags/server_setup.html">server setup</a> (1), <a href="../../../tags/social_networks.html">social networks</a> (1), <a href="../../../tags/sphinx.html">sphinx</a> (1), <a href="../../../tags/sql.html">sql</a> (1), <a href="../../../tags/sqlite3.html">sqlite3</a> (1), <a href="../../../tags/ssh.html">ssh</a> (1), <a href="../../../tags/ssh_keys.html">ssh keys</a> (1), <a href="../../../tags/statsmodels.html">statsmodels</a> (1), <a href="../../../tags/sympy.html">sympy</a> (1), <a href="../../../tags/tableau.html">tableau</a> (1), <a href="../../../tags/tinkerer.html">tinkerer</a> (1), <a href="../../../tags/topic_models.html">topic models</a> (1), <a href="../../../tags/tree.html">tree</a> (1), <a href="../../../tags/ubuntu_14_04.html">ubuntu 14.04</a> (7), <a href="../../../tags/vim.html">vim</a> (1), <a href="../../../tags/virtualbox.html">virtualbox</a> (1), <a href="../../../tags/virtualenv.html">virtualenv</a> (3), <a href="../../../tags/virtualenvwrapper.html">virtualenvwrapper</a> (2), <a href="../../../tags/vps.html">VPS</a> (1), <a href="../../../tags/yaml.html">yaml</a> (1)</div></section></aside></div> <!-- #main --></div> <!-- #main-container -->

        <div class="footer-container" role="contentinfo"><footer class="wrapper">&copy; Copyright 2014 Christopher Strelioff. Powered by <a href="http://www.tinkerer.me/">Tinkerer</a> and <a href="http://sphinx.pocoo.org/">Sphinx</a>.</footer></div> <!-- footer-container -->

      </div> <!--! end of #container --><!--[if lt IE 7 ]>
          <script src="//ajax.googleapis.com/ajax/libs/chrome-frame/1.0.3/CFInstall.min.js"></script>
          <script>window.attachEvent('onload',function(){CFInstall.check({mode:'overlay'})})</script>
        <![endif]-->
    </body>
</html>